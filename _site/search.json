[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "testblog",
    "section": "",
    "text": "Hate Speech auf Twitter\n\n\n\n\n\n\n\nTextanalyse\n\n\nKlassifikation\n\n\nHuggingface\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nRaphael Balzer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html",
    "href": "posts/Hatespeech_Twitter.html",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel dieses Posts ist es, Hate Speech auf Twitter zu klassifizieren. Hass im Internet ist nach wie vor ein gro√ües gesellschaftliches Problem, weshalb es sich lohnt, genauer zu untersuchen, was diesen Hass ausmacht und wie man ihn zuverl√§ssig und automatisiert erkennen kann. Hierf√ºr liegt ein Datensatz vor, der eine Auswahl englischer als Hate Speech oder nicht Hate Speech markierter Tweets enth√§lt. Die Analyse dieser Daten l√§sst sich in zwei Teile gliedern: Zun√§chst werden Methoden der explorativen Datenanalyse angewandt, um Muster und Auff√§lligkeiten in den Tweets zu identifizieren. Anschlie√üend werden die gewonnenen Erkenntnisse genutzt, um sowohl Shallow-Learning- als auch Deep-Learning-Algorithmen darauf zu trainieren, Tweets korrekterweise als Hate Speech einzuordnen.\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidytext)\nlibrary(syuzhet)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(stringr)\nlibrary(readr)\nlibrary(vip)\n\n\n\n\nBei den Daten handelt es sich um eine Auswahl englischer Tweets, die bereits auf Hate Speech untersucht wurden und sich daher gut f√ºr das Training von Modellen zur Erkennung von Hassrede eignen.\n\nd_hate &lt;- read_csv(\"d_hate.csv\")\n\n\n\n\n\nZiel ist es, auf Grundlage der Tweets einige n√ºtzliche Features zu generieren, die sich als Pr√§diktor f√ºr die AV (Hatespeech oder nicht) eignen. Hierf√ºr m√ºssen zun√§chst einige Charakteristika von Tweets, die als Hate Speech gelten, herausgearbeitet werden. Die Methoden und der Code orientieren sich stark an dem Vorgehen, das in Julia Silges und David Robinsons Buch ‚ÄúText Mining with R‚Äù (https://www.tidytextmining.com/) beschrieben wird. F√ºr die nachfolgenden Visualisierungen wird eine mit Hilfe der Seite https://davidmathlogic.com/colorblind eigens erstellte Farbpalette verwendet, die gew√§hrleistet, dass keine Art der Farbenblindheit die Lesbarkeit der Diagramme beeintr√§chtigt.\n\nUriah_Flint &lt;- c(\"#8175AA\", \"#6FB899\", \"#3AA2C3\", \"#8BD4F9\", \"#DDCC77\", \"#CC6677\", \"#882255\")\n\n\n\nUm eine sinnvolle Analyse durchzuf√ºhren, m√ºssen noch einige Datenvorverarbeitungsschritte durchlaufen werden. Diese beinhalten die Tokenisierung, das Entfernen von Stopwords und das Bereinigen der Tweets, die Links oder √§hnliche Elemente enthalten.\n\nd_hate_clean &lt;- d_hate %&gt;%\n  mutate(tweet = str_remove_all(tweet, pattern = 'RT\\\\s*|http[s]?://\\\\S+|\\\\d+'))\n\nset.seed(123)\ntrain_test_split &lt;- initial_split(d_hate_clean, prop = .8, strata = class)\nd_train &lt;- training(train_test_split)\nd_test &lt;- testing(train_test_split)\n\n\n\n\ntweets_token &lt;- d_train %&gt;%\n  unnest_tokens(word, tweet)\n\n\n\n\n\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\n\ntweets_token &lt;- tweets_token %&gt;%\n  anti_join(stopwords_en)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nsenti &lt;- get_sentiments(\"afinn\") %&gt;% \n  mutate(neg_pos = case_when(value &gt; 0 ~ \"pos\",\n                             TRUE ~ \"neg\"))\n\ntweets_senti &lt;- tweets_token %&gt;%\ninner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nUm sich einen ersten √úberblick √ºber die Daten zu verschaffen, ist es sinnvoll, zun√§chst einmal den Anteil der als Hate Speech markierten Tweets zu √ºberpr√ºfen.\n\ntweets_token %&gt;% \n  summarise(`Anteil Hate Speech` = mean(class == \"hate speech\")) %&gt;% \n  round(2)\n\n# A tibble: 1 √ó 1\n  `Anteil Hate Speech`\n                 &lt;dbl&gt;\n1                 0.25\n\n\n\nclass_totals &lt;- tweets_token %&gt;%\n  count(class, name = \"class_total\")\n\nggplot(class_totals, aes(x = \"\", y = class_total, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil an Hate Speech\",\n       x = NULL,\n       y = NULL,\n       fill = \"Klasse\") +\n  geom_text(aes(label = class_total), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nDer Anteil der Hate Speech in diesem Datensatz betr√§gt 25 Prozent. Die Tweets anderer Kategorien sind also deutlich in der Mehrheit.\n\n\n\nEinen weiteren interessanten Einblick gew√§hren die Worth√§ufigkeiten. Durch die Visualisierung der am meisten verwendeten W√∂rter und Wortpaare ist es bereits m√∂glich, einen Einblick in das Vokabular zu erhalten und dieses unter den Klassen zu vergleichen.\n\ntweets_count_senti &lt;- tweets_senti %&gt;%\n  group_by(class) %&gt;% \n  count(class, word, sort = TRUE) %&gt;% \n  slice_head(n = 10)\n\n# Zusammenf√ºhren von \"word_counts\" und \"class_totals\" nach der Klasse\nword_counts &lt;- left_join(tweets_count_senti, class_totals, by = \"class\")\n\n# Berechnung der gewichteten H√§ufigkeit\nword_counts &lt;- word_counts %&gt;%\n  mutate(weighted_frequency = n / class_total)\n\n# Visualisierung der gewichteten H√§ufigkeiten\nggplot(word_counts, aes(x = reorder(word, weighted_frequency), y = weighted_frequency, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip() +\n  labs(title = \"Gewichtete H√§ufigkeiten der W√∂rter in Abh√§ngigkeit von der Klasse\",\n       x = \"Wort\",\n       y = \"Gewichtete H√§ufigkeit\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nBeim Vergleich der h√§ufigsten W√∂rter f√§llt direkt auf, dass Beleidigungen und Schimpfw√∂rter charakteristisch f√ºr Hate Speech sind, da die Liste der zehn h√§ufigsten W√∂rter fast nur aus solchen Eintr√§gen besteht. Das Vokabular der anderen Kategorie ist im Vergleich dazu √ºberaus harmlos. Diese Harmlosigkeit wird durch das h√§ufigste Wort ‚Äúlol‚Äù noch auf die Spitze getrieben. Interessant ist jedoch auch, dass sich das Wort ‚Äúhate‚Äù in dieser Liste wiederfindet. Hier w√§re es interessant, im weiteren Verlauf der Analyse den Kontext in Erfahrung zu bringen. Auf der anderen Seite ist ‚Äúhate‚Äù jedoch ein sehr g√§ngiges Wort und dient zur Beschreibung normaler Gef√ºhlszust√§nde, ohne direkt Hass zu verbreiten.\n\ntweets_bigram &lt;- \n  d_train %&gt;%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ntweets_bigram &lt;- tweets_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntweets_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  group_by(class) %&gt;% \n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram, fill = class) ) +\n  facet_wrap(~class, scales = \"free_y\") +\n  geom_col() +\n  labs(title = \"Bigramme nach H√§ufigkeit\",\n       x = \"H√§ufigkeit\",\n       y = \"Bigramm\") +\n  scale_fill_manual(values = Uriah_Flint) +\n  theme_light()\n\n\n\n\nDie Analyse der h√§ufigsten Wortpaare deckt sich mit der Analyse der h√§ufigsten W√∂rter. Sie bringt insofern neue Erkenntnisse, als deutlich wird, dass sich der Hass haupts√§chlich gegen ethnische und sexuelle Minderheiten richtet. Dies wird anhand von Begriffen wie ‚Äúwhite trash‚Äù und ‚Äúfucking faggot‚Äù deutlich. Bemerkenswert ist ebenfalls, dass es sich keinesfalls haupts√§chlich um Hass gegen Schwarze handelt, sondern genauso auch Menschen mit heller Hautfarbe ethnisch beleidigt werden.\n\n\n\nIm Folgenden werden alle Wortpaare, die h√§ufiger als sechs Mal vorkommen, visualisiert. Hierdurch werden die Kontexte der W√∂rter deutlicher und die Beziehungen k√∂nnen uns Aufschluss dar√ºber geben, in welchem Zusammenhang ‚Äúhate‚Äù verwendet wird.\n\ntweets_bigram_count &lt;- tweets_bigram %&gt;% \n   count(word1, word2, sort = TRUE)\n\nvisualize_bigrams &lt;- function(bigrams) {\n  set.seed(2016)\n  a &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n  \n  bigrams %&gt;%\n    graph_from_data_frame() %&gt;%\n    ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +\n    geom_node_point(color = \"#6FB899\", size = 5) +\n    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n    theme_void()\n}\n\ntweets_bigram_count %&gt;%\n  filter(n &gt; 6,\n         !str_detect(word1, \"\\\\d\"),\n         !str_detect(word2, \"\\\\d\")) %&gt;%\n  visualize_bigrams()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nZum Kontext des Wortes ‚Äúhate‚Äù erhalten wir hier keine neuen Hinweise. Jedoch wird klar ersichtlich, in welchen Kombinationen Schimpfw√∂rter verwendet werden. Au√üerdem werden Ambiguit√§ten deutlich, da W√∂rter wie ‚Äútrash‚Äù und ‚Äúcolored‚Äù sowohl als rassistische Beleidigung als auch als Beschreibung von Alltagsgegenst√§nden auftauchen.\n\n\n\nZweck der Sentimentanalyse ist es, herauszufinden, ob die Sentimentauspr√§gungen die beiden Klassen klar voneinander abgrenzen.\n\n# Z√§hlen der negativen und positiven Sentimente\ntweets_senti2 &lt;- tweets_senti %&gt;% \n  group_by(class) %&gt;% \n  count(neg_pos, name = \"count\")\n\n# Visualisierung der Sentimentantanteile nach Klasse\nggplot(tweets_senti2, aes(x = \"\", y = count, fill = neg_pos)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sentimentanteile nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Sentiment\") +\n  facet_wrap(~ class) +\n  geom_text(aes(label = count), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nIn obigem Diagramm wird ersichtlich, dass hasslastige Tweets √ºberwiegend negativ sind, w√§hrend sich die Sentimente der anderen Klasse in der Waage halten. Das Sentiment ist also ein entscheidender Faktor bei der Klassifizierung von Hate Speech und sollte beim Training des Modells ber√ºcksichtigt werden.\n\n\n\nDie Themenanalyse soll Aufschluss dar√ºber geben, ob es bestimmte Themengebiete gibt, die charakteristisch f√ºr Hate Speech sind.\n\ntweets_token_counts_hate &lt;- tweets_token %&gt;%\n  filter(class == \"hate speech\") %&gt;% \n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 19) %&gt;% \n  select(word)\n\ntweets_dtm_hate &lt;- DocumentTermMatrix(tweets_token_counts_hate)\ntweets_dtm_hate\n\n\ntweets_lda_hate &lt;- LDA(tweets_dtm_hate, k = 4, control = list(seed = 42))\n\ntweets_themen_hate &lt;- tidy(tweets_lda_hate, matrix = \"beta\")\n\ntweets_themen_hate &lt;- tweets_themen_hate %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 7) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntweets_themen_hate %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(title = \"Themen von Hate Speech\") +\n  theme_minimal() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nObwohl sich die Themen nicht eindeutig voneinander abgrenzen, sind dennoch schwache Muster erkennbar. Thema Eins scheint sich vor allem aus allgemeinen Obsz√∂nit√§ten zusammenzusetzen, w√§hrend das zweite Thema aus Beleidigungen gegen Schwule und Schwarze und etwas h√§rteren W√∂rtern wie ‚Äúkill‚Äù und ‚Äúshit‚Äù besteht. In Thema Drei und Vier treten Beleidigungen gegen Frauen sowie die LGBTQ-Community in den Vordergrund. Viel wichtiger als diese kleinen Unterschiede ist jedoch das gro√üe Bild der Themen, welches wie schon bei der Analyse der Worth√§ufigkeiten festgestellt, haupts√§chlich aus ethnischen und sexuellen Beleidigungen und Schimpfw√∂rtern besteht.\n\n\n\nSchimpfw√∂rter scheinen eine gro√üe Rolle bei Hate Speech zu spielen. Deshalb erachte ich es als sinnvoll, Schimpfw√∂rter als Feature in das sp√§tere Rezept mit aufzunehmen. Hierzu verwende ich diese Liste, welche ich um ein paar Eintr√§ge (rassistische Beleidigungen) erg√§nzt habe: https://www.insult.wiki/list-of-insults.\n\ninsults &lt;- read.csv(\"insults.csv\")\n\ntweets_token %&gt;%\n  group_by(class) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(insults, by = \"word\") %&gt;% \n  mutate(insult = case_when(is.na(value) == TRUE ~ \"Nein\",\n                            TRUE ~ \"Ja\")) %&gt;% \n  select(-value) %&gt;% \nggplot(aes(x = \"\", y = n, fill = insult)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil der Beleidigungen nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Beleidigung\") +\n  facet_wrap(~ class, scales = \"free_y\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nTats√§chlich ist der Anteil der Beleidigungen in Hate Speech Tweets h√∂her, jedoch f√§llt er deutlich geringer aus als erwartet.\n\n\n\nDie √úberlegung, dass Hate Speech feindselige Emojis enth√§lt, ist sehr plausibel. Um aggressive Emojis zu kennzeichnen, verwende ich ein von mir erstelltes Lexikon, das solche Emojis enth√§lt und schreibe eine Funktion, die z√§hlt, wieviele feindselige Emojis in einem Tweet vorkommen. Der Totenkopf ist nicht in der Liste der feindseligen Emojis enthalten, da dieser haupts√§chlich als Synonym oder Steigerung des Lach-Emojis verwendet wird (engl.: ‚ÄúThat‚Äôs too funny. I‚Äôm dead!‚Äù).\n\nhostile_emojis &lt;- read.csv(\"hostile_emojis.csv\")\n\n\ncount_hostile_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor f√ºr die Z√§hlungen\n  counts &lt;- numeric(length(hostile_emojis$emoji))\n\n  # Iteriere √ºber jedes Emoji und z√§hle die √úbereinstimmungen im Text\n  for (i in seq_along(hostile_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, hostile_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der √úbereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"üóë\", \"bogen\", \"üò†\", \"üëπ\", \"üí©\", \"baby\", \"und\", \"üÜó\")\ncount_hostile_emojis(dummy)\n\n[1] 3\n\n\n\nd_train %&gt;% \n  mutate(hostile_emojis_n = map_int(tweet, count_hostile_emojis)) %&gt;% \n  summarise(`Feindselige Emojis` = mean(hostile_emojis_n == 1))\n\n# A tibble: 1 √ó 1\n  `Feindselige Emojis`\n                 &lt;dbl&gt;\n1                    0\n\n\nDie Vermutung, dass Hate Speech feindselige Emojis enth√§lt, stellt sich in diesem Fall als falsch heraus. Da es keinen einzigen Emoji dieser Art gibt, wird dieser Ansatz f√ºr die Modellierung verworfen.\n\n\n\n\nIn der Modellierung ist es nun das Ziel, einen Algorithmus darauf zu trainieren, m√∂glichst pr√§zise Hate Speech vorherzusagen. Der Algorithmus meiner Wahl ist der XGBoost. Zun√§chst werden jedoch noch Rezepte formuliert, die die Erkenntnisse aus der Analyse nun in n√ºtzliche Features umwandeln.\n\n\nRezept Eins enth√§lt Schimpfw√∂rter, Sentimentwerte und die Themenanalyse. Au√üerdem werden noch die √ºblichen Textverarbeitungsschritte durchgef√ºhrt sowie ein Tokenfilter angewandt.\n\nrec1 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;%\n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_lda(tweet, num_topics = 6)\n\nRezept Zwei enth√§lt statt der Themenanalyse die Tf-idf-Ma√üe.\n\nrec2 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_tfidf(tweet)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n# A tibble: 4,474 √ó 10\n      id class      insult senti lda_tweet_1 lda_tweet_2 lda_tweet_3 lda_tweet_4\n   &lt;dbl&gt; &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1    85 hate spee‚Ä¶      1 -0.25       0            0           0            0  \n 2    90 hate spee‚Ä¶      1  1          0.1          0.3         0            0.6\n 3   186 hate spee‚Ä¶      1 -1          0.433        0.4         0            0.1\n 4   204 hate spee‚Ä¶      1 -0.75       0            0           0.3          0.2\n 5   206 hate spee‚Ä¶      2 -0.75       0            0.3         0.5          0  \n 6   221 hate spee‚Ä¶      0  1.4        0            0           0            0.2\n 7   263 hate spee‚Ä¶      1 -1.75       0            0           0.1          0.4\n 8   317 hate spee‚Ä¶      1 -0.55       0            0           0.5          0  \n 9   320 hate spee‚Ä¶      0 -0.35       0.1          0.1         0            0.2\n10   354 hate spee‚Ä¶      1 -0.75       0.25         0.15        0.05         0.5\n# ‚Ñπ 4,464 more rows\n# ‚Ñπ 2 more variables: lda_tweet_5 &lt;dbl&gt;, lda_tweet_6 &lt;dbl&gt;\n\n\n\nbaked2 &lt;- rec2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n# A tibble: 4,474 √ó 44\n      id class     insult senti tfidf_tweet_amp tfidf_tweet_ass tfidf_tweet_back\n   &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1    85 hate spe‚Ä¶      1 -0.25               0               0                0\n 2    90 hate spe‚Ä¶      1  1                  0               0                0\n 3   186 hate spe‚Ä¶      1 -1                  0               0                0\n 4   204 hate spe‚Ä¶      1 -0.75               0               0                0\n 5   206 hate spe‚Ä¶      2 -0.75               0               0                0\n 6   221 hate spe‚Ä¶      0  1.4                0               0                0\n 7   263 hate spe‚Ä¶      1 -1.75               0               0                0\n 8   317 hate spe‚Ä¶      1 -0.55               0               0                0\n 9   320 hate spe‚Ä¶      0 -0.35               0               0                0\n10   354 hate spe‚Ä¶      1 -0.75               0               0                0\n# ‚Ñπ 4,464 more rows\n# ‚Ñπ 37 more variables: tfidf_tweet_bird &lt;dbl&gt;, tfidf_tweet_birds &lt;dbl&gt;,\n#   tfidf_tweet_bitch &lt;dbl&gt;, tfidf_tweet_can &lt;dbl&gt;, tfidf_tweet_charlie &lt;dbl&gt;,\n#   tfidf_tweet_colored &lt;dbl&gt;, tfidf_tweet_day &lt;dbl&gt;, tfidf_tweet_faggot &lt;dbl&gt;,\n#   tfidf_tweet_fuck &lt;dbl&gt;, tfidf_tweet_get &lt;dbl&gt;, tfidf_tweet_ghetto &lt;dbl&gt;,\n#   tfidf_tweet_go &lt;dbl&gt;, tfidf_tweet_good &lt;dbl&gt;, tfidf_tweet_got &lt;dbl&gt;,\n#   tfidf_tweet_hate &lt;dbl&gt;, tfidf_tweet_just &lt;dbl&gt;, tfidf_tweet_know &lt;dbl&gt;, ‚Ä¶\n\n\n\n\n\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n\nDas Modell wird getuned. Hierf√ºr wird zweifache Kreuzvalidierung mit einer Wiederholung verwendet. Der geringe Performance-Zuwachs durch intensiveres Tuning mit mehr Folds und Wiederholungen w√ºrde in diesem Fall nicht die h√∂here Rechenzeit rechtfertigen.\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n\ni 1 of 2 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n‚úî 1 of 2 tuning:     rec1_xgb (1m 13.7s)\n\n\ni 2 of 2 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n‚úî 2 of 2 tuning:     rec2_xgb (1m 25s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 28 √ó 9\n   wflow_id .config         preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.904     2 0.00857\n 2 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.881     2 0.0115 \n 3 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.880     2 0.00246\n 4 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.868     2 0.0196 \n 5 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.866     2 0.00112\n 6 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.855     2 0.00983\n 7 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.854     2 0.0115 \n 8 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.853     2 0.0140 \n 9 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.852     2 0.0167 \n10 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.848     2 0.00201\n# ‚Ñπ 18 more rows\n\n\nRezept Zwei hat besser abgeschnitten als Rezept Eins. W√§hlen wir nun das beste Modell aus und fitten es:\n\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec2_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec2_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\nDie Analyse der wichtigsten Pr√§diktoren deckt sich mit den Erkenntnissen aus der EDA. Die mit Abstand wichtigsten Features sind die Beleidigungen und Sentimentwerte, w√§hrend die Tf-idf-Ma√üe von Beleidigungen ebenfalls viel zur Prediction beitragen.\n\nwf_preds &lt;-\n  collect_predictions(model_set)\n\nwf_preds %&gt;%\n  group_by(wflow_id) %&gt;% \n  roc_curve(truth = class, `.pred_hate speech`) %&gt;% \n  autoplot()\n\n\n\n\nDie Performance im Train-Sample f√§llt sehr gut aus, da die Vorhersagen mit einer Genauigkeit von rund 90 Prozent sehr pr√§zise sind.\n\n\n\n\n\npreds &lt;- predict(fit_final, d_test)\npreds\n\n# A tibble: 1,119 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 other      \n 2 other      \n 3 hate speech\n 4 other      \n 5 hate speech\n 6 other      \n 7 other      \n 8 other      \n 9 other      \n10 other      \n# ‚Ñπ 1,109 more rows\n\n\n\n\n\nd_test1 &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(class = as.factor(class))\nd_test1\n\n# A tibble: 1,119 √ó 4\n      id tweet                                                 class .pred_class\n   &lt;dbl&gt; &lt;chr&gt;                                                 &lt;fct&gt; &lt;fct&gt;      \n 1    66 \"\\\"@AllAboutManFeet:  woof woof and hot soles\"        other other      \n 2    70 \"\\\"@ArizonasFinest: Why the eggplant emoji doe?\\\"y h‚Ä¶ other other      \n 3   111 \"\\\"@DevilGrimz: @VigxRArts you're fucking gay, black‚Ä¶ hate‚Ä¶ hate speech\n 4   116 \"\\\"@DomWorldPeace: Baseball season for the win. #Yan‚Ä¶ other other      \n 5   151 \"\\\"@JReebo: Who wants to get there nose in these bad‚Ä¶ other hate speech\n 6   192 \"\\\"@Mesha_nojas: @_Vontethekidd &#;&#;&#;&#;&#;\\\" I ‚Ä¶ other other      \n 7   245 \"\\\"@Stephenwildboy: Chilling &#;&#;  sexy scally lad\" other other      \n 8   251 \"\\\"@TEE_JONEZ: @KingCuh @stanleys I'm not mad cuh ha‚Ä¶ other other      \n 9   318 \"\\\"@ayyee_ceee_: One mans trash &#; is another mans ‚Ä¶ other other      \n10   341 \"\\\"@hxhassan: Jihadi patron Hajjaj al-Ajmi is arrest‚Ä¶ other other      \n# ‚Ñπ 1,109 more rows\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test1,\n           truth = class,\n           estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.883\n2 f_meas   binary         0.744\n\n\nAuch im Test-Sample bew√§hrt sich das Modell mit einer sehr hohen Genauigkeit.\n\n\n\n\nEin weiterer Ansatz zur Klassifikation von Hate Speech ist es, kein eigenes Modell zu trainieren, sondern Zero-Shot-Learning anzuwenden. Das ergibt nat√ºrlich am meisten Sinn mit einem sehr fortgeschrittenen und komplexen Transformer-Modell, das bereits auf die Erkennung von Hate Speech trainiert wurde. Im Folgenden wird daher das Modell roberta-hate-speech-dynabench-r4-target von Facebook, welches auf Huggingface.co verf√ºgbar ist, um die Tweets nach Hate Speech zu klassifizieren. Hierzu wird der Befehl pipeline aus der transformers-Library von Huggingface genutzt, um das Modell zu laden und auf das Test-Sample anzuwenden.\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.2.3\n\n\n\nuse_virtualenv(\"C:/Users/rapha/venv\")\n\n\nfrom transformers import pipeline\nimport tensorflow as tf\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n\n\n\ntweets &lt;- d_test$tweet\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult &lt;- py$results\nlabels &lt;- lapply(result, function(element) element$label)\ntweets_hate &lt;- cbind(d_test, pred = unlist(labels))\ntweets_hate &lt;- tweets_hate %&gt;% \n  mutate(class = as.factor(class),\n         pred = case_when(pred == \"hate\" ~ \"hate speech\",\n            pred == \"nothate\" ~ \"other\"),\n         pred = as.factor(pred))\n\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(tweets_hate,\n           truth = class,\n           estimate = pred)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.882\n2 f_meas   binary         0.797\n\n\nDie Performance des Modells ist objektiv gesehen gut, verglichen mit dem XGBoost mit einer Minute Trainingszeit f√§llt sie jedoch mager aus.\n\n\n\nBisher wurde Hate Speech sowohl mit Hilfe eines auf den konkreten Daten trainierten Shallow-Learner als auch mit Hilfe eines vortrainierten Transformers klassifiziert. Im letzten Schritt dieses Posts sollen die St√§rken dieser beiden Ans√§tze kombiniert werden, indem ein Deep-Learning-Algorithums, genauer gesagt ein Neuronales Netzwerk, auf den vorliegenden Daten trainiert wird. Das neuronale Netz verwendet ein vortrainiertes Wort-Einbettungsmodell mit 50 Dimensionen, das f√ºr die deutsche Sprache optimiert ist. Dieses Embedding-Modell erm√∂glicht es dem Netzwerk, semantische Repr√§sentationen der W√∂rter zu erlernen. Das Netzwerk besteht aus einer Eingabeschicht, die das Embedding-Modell enth√§lt, gefolgt von einer vollst√§ndig verbundenen Schicht mit 32 Neuronen und einer Sigmoid-Aktivierungsfunktion. Weiterhin gibt es eine Schicht mit 24 Neuronen und einer ReLU-Aktivierung. Die Ausgabeschicht besteht aus einem einzelnen Neuron f√ºr bin√§re Klassifikation. Das Netzwerk wird mit dem Adam-Optimizer kompiliert und die bin√§re Kreuzentropie wird als Verlustfunktion verwendet. Die Accuracy wird als Metrik √ºberwacht. Das Training erfolgt √ºber 3 Epochen mit einer Batch-Gr√∂√üe von 48, wobei die Validierung anhand des Test-Samples durchgef√ºhrt wird.\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n\nd_train = r.d_train\nd_test = r.d_test\n\nX_train = d_train[\"tweet\"].values\nX_test = d_test[\"tweet\"].values\n\n\nd_train[\"y\"] = d_train[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_train = d_train.loc[:, \"y\"].values\n\nd_test[\"y\"] = d_test[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_test = d_test.loc[:, \"y\"].values\n\n\nembedding = \"https://tfhub.dev/google/nnlm-de-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\ntf.random.set_seed(42)\n\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(32, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(24, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n\n\nmodel.fit(X_train, y_train,\nepochs=3,\nbatch_size=48,\nvalidation_data=(X_test, y_test),\nverbose = 1)\n\nEpoch 1/3\n\n 1/94 [..............................] - ETA: 3:06 - loss: 0.6014 - accuracy: 0.7500\n 2/94 [..............................] - ETA: 1:45 - loss: 0.5919 - accuracy: 0.7604\n 3/94 [..............................] - ETA: 1:05 - loss: 0.5953 - accuracy: 0.7500\n 4/94 [&gt;.............................] - ETA: 51s - loss: 0.6090 - accuracy: 0.7240 \n 5/94 [&gt;.............................] - ETA: 46s - loss: 0.6009 - accuracy: 0.7333\n 6/94 [&gt;.............................] - ETA: 42s - loss: 0.5789 - accuracy: 0.7604\n 7/94 [=&gt;............................] - ETA: 38s - loss: 0.5799 - accuracy: 0.7560\n 8/94 [=&gt;............................] - ETA: 37s - loss: 0.5771 - accuracy: 0.7578\n 9/94 [=&gt;............................] - ETA: 35s - loss: 0.5706 - accuracy: 0.7639\n10/94 [==&gt;...........................] - ETA: 33s - loss: 0.5739 - accuracy: 0.7583\n11/94 [==&gt;...........................] - ETA: 31s - loss: 0.5629 - accuracy: 0.7689\n12/94 [==&gt;...........................] - ETA: 30s - loss: 0.5633 - accuracy: 0.7674\n13/94 [===&gt;..........................] - ETA: 29s - loss: 0.5618 - accuracy: 0.7676\n14/94 [===&gt;..........................] - ETA: 28s - loss: 0.5620 - accuracy: 0.7664\n15/94 [===&gt;..........................] - ETA: 27s - loss: 0.5646 - accuracy: 0.7625\n16/94 [====&gt;.........................] - ETA: 26s - loss: 0.5658 - accuracy: 0.7604\n17/94 [====&gt;.........................] - ETA: 25s - loss: 0.5680 - accuracy: 0.7574\n18/94 [====&gt;.........................] - ETA: 24s - loss: 0.5713 - accuracy: 0.7535\n19/94 [=====&gt;........................] - ETA: 24s - loss: 0.5768 - accuracy: 0.7478\n20/94 [=====&gt;........................] - ETA: 23s - loss: 0.5782 - accuracy: 0.7458\n21/94 [=====&gt;........................] - ETA: 23s - loss: 0.5752 - accuracy: 0.7480\n22/94 [======&gt;.......................] - ETA: 22s - loss: 0.5745 - accuracy: 0.7481\n23/94 [======&gt;.......................] - ETA: 21s - loss: 0.5710 - accuracy: 0.7509\n24/94 [======&gt;.......................] - ETA: 21s - loss: 0.5734 - accuracy: 0.7483\n25/94 [======&gt;.......................] - ETA: 20s - loss: 0.5730 - accuracy: 0.7483\n26/94 [=======&gt;......................] - ETA: 20s - loss: 0.5791 - accuracy: 0.7420\n27/94 [=======&gt;......................] - ETA: 19s - loss: 0.5833 - accuracy: 0.7377\n28/94 [=======&gt;......................] - ETA: 19s - loss: 0.5817 - accuracy: 0.7388\n29/94 [========&gt;.....................] - ETA: 19s - loss: 0.5863 - accuracy: 0.7342\n30/94 [========&gt;.....................] - ETA: 18s - loss: 0.5826 - accuracy: 0.7375\n31/94 [========&gt;.....................] - ETA: 18s - loss: 0.5797 - accuracy: 0.7399\n32/94 [=========&gt;....................] - ETA: 17s - loss: 0.5784 - accuracy: 0.7409\n33/94 [=========&gt;....................] - ETA: 17s - loss: 0.5765 - accuracy: 0.7424\n34/94 [=========&gt;....................] - ETA: 17s - loss: 0.5786 - accuracy: 0.7402\n35/94 [==========&gt;...................] - ETA: 16s - loss: 0.5769 - accuracy: 0.7417\n36/94 [==========&gt;...................] - ETA: 16s - loss: 0.5759 - accuracy: 0.7425\n37/94 [==========&gt;...................] - ETA: 16s - loss: 0.5762 - accuracy: 0.7421\n38/94 [===========&gt;..................] - ETA: 15s - loss: 0.5735 - accuracy: 0.7445\n39/94 [===========&gt;..................] - ETA: 15s - loss: 0.5730 - accuracy: 0.7447\n40/94 [===========&gt;..................] - ETA: 15s - loss: 0.5714 - accuracy: 0.7458\n41/94 [============&gt;.................] - ETA: 14s - loss: 0.5705 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 14s - loss: 0.5680 - accuracy: 0.7485\n43/94 [============&gt;.................] - ETA: 14s - loss: 0.5703 - accuracy: 0.7461\n44/94 [=============&gt;................] - ETA: 13s - loss: 0.5705 - accuracy: 0.7457\n45/94 [=============&gt;................] - ETA: 13s - loss: 0.5711 - accuracy: 0.7449\n46/94 [=============&gt;................] - ETA: 13s - loss: 0.5709 - accuracy: 0.7450\n47/94 [==============&gt;...............] - ETA: 12s - loss: 0.5692 - accuracy: 0.7465\n48/94 [==============&gt;...............] - ETA: 12s - loss: 0.5684 - accuracy: 0.7470\n49/94 [==============&gt;...............] - ETA: 12s - loss: 0.5696 - accuracy: 0.7457\n50/94 [==============&gt;...............] - ETA: 11s - loss: 0.5694 - accuracy: 0.7458\n51/94 [===============&gt;..............] - ETA: 11s - loss: 0.5687 - accuracy: 0.7463\n52/94 [===============&gt;..............] - ETA: 11s - loss: 0.5680 - accuracy: 0.7468\n53/94 [===============&gt;..............] - ETA: 11s - loss: 0.5672 - accuracy: 0.7472\n54/94 [================&gt;.............] - ETA: 10s - loss: 0.5679 - accuracy: 0.7465\n55/94 [================&gt;.............] - ETA: 10s - loss: 0.5706 - accuracy: 0.7439\n56/94 [================&gt;.............] - ETA: 10s - loss: 0.5719 - accuracy: 0.7426\n57/94 [=================&gt;............] - ETA: 9s - loss: 0.5700 - accuracy: 0.7442 \n58/94 [=================&gt;............] - ETA: 9s - loss: 0.5690 - accuracy: 0.7450\n59/94 [=================&gt;............] - ETA: 9s - loss: 0.5687 - accuracy: 0.7451\n60/94 [==================&gt;...........] - ETA: 9s - loss: 0.5685 - accuracy: 0.7451\n61/94 [==================&gt;...........] - ETA: 8s - loss: 0.5676 - accuracy: 0.7459\n62/94 [==================&gt;...........] - ETA: 8s - loss: 0.5666 - accuracy: 0.7466\n63/94 [===================&gt;..........] - ETA: 8s - loss: 0.5682 - accuracy: 0.7450\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7441\n65/94 [===================&gt;..........] - ETA: 7s - loss: 0.5692 - accuracy: 0.7439\n66/94 [====================&gt;.........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7440\n67/94 [====================&gt;.........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7438\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.5688 - accuracy: 0.7439\n69/94 [=====================&gt;........] - ETA: 6s - loss: 0.5686 - accuracy: 0.7440\n70/94 [=====================&gt;........] - ETA: 6s - loss: 0.5689 - accuracy: 0.7435\n71/94 [=====================&gt;........] - ETA: 6s - loss: 0.5689 - accuracy: 0.7433\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.5700 - accuracy: 0.7419\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.5694 - accuracy: 0.7423\n74/94 [======================&gt;.......] - ETA: 5s - loss: 0.5692 - accuracy: 0.7424\n75/94 [======================&gt;.......] - ETA: 5s - loss: 0.5699 - accuracy: 0.7417\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.5711 - accuracy: 0.7404\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.5716 - accuracy: 0.7397\n78/94 [=======================&gt;......] - ETA: 4s - loss: 0.5722 - accuracy: 0.7390\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.5719 - accuracy: 0.7392\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.5714 - accuracy: 0.7396\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.5700 - accuracy: 0.7410\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.5702 - accuracy: 0.7406\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.5699 - accuracy: 0.7407\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.5696 - accuracy: 0.7408\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.5689 - accuracy: 0.7414\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.5680 - accuracy: 0.7422\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.5684 - accuracy: 0.7416\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.5689 - accuracy: 0.7410\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.5678 - accuracy: 0.7420\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.5664 - accuracy: 0.7433\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7443\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7434\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7440\n94/94 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.7443\n94/94 [==============================] - 27s 272ms/step - loss: 0.5647 - accuracy: 0.7443 - val_loss: 0.5486 - val_accuracy: 0.7444\nEpoch 2/3\n\n 1/94 [..............................] - ETA: 21s - loss: 0.6380 - accuracy: 0.6458\n 2/94 [..............................] - ETA: 21s - loss: 0.5990 - accuracy: 0.6875\n 3/94 [..............................] - ETA: 20s - loss: 0.5941 - accuracy: 0.6944\n 4/94 [&gt;.............................] - ETA: 21s - loss: 0.5791 - accuracy: 0.7083\n 5/94 [&gt;.............................] - ETA: 20s - loss: 0.5733 - accuracy: 0.7125\n 6/94 [&gt;.............................] - ETA: 21s - loss: 0.5583 - accuracy: 0.7257\n 7/94 [=&gt;............................] - ETA: 20s - loss: 0.5513 - accuracy: 0.7321\n 8/94 [=&gt;............................] - ETA: 20s - loss: 0.5540 - accuracy: 0.7292\n 9/94 [=&gt;............................] - ETA: 20s - loss: 0.5646 - accuracy: 0.7199\n10/94 [==&gt;...........................] - ETA: 20s - loss: 0.5613 - accuracy: 0.7229\n11/94 [==&gt;...........................] - ETA: 20s - loss: 0.5524 - accuracy: 0.7311\n12/94 [==&gt;...........................] - ETA: 20s - loss: 0.5463 - accuracy: 0.7361\n13/94 [===&gt;..........................] - ETA: 20s - loss: 0.5535 - accuracy: 0.7292\n14/94 [===&gt;..........................] - ETA: 20s - loss: 0.5599 - accuracy: 0.7217\n15/94 [===&gt;..........................] - ETA: 20s - loss: 0.5504 - accuracy: 0.7306\n16/94 [====&gt;.........................] - ETA: 20s - loss: 0.5449 - accuracy: 0.7357\n17/94 [====&gt;.........................] - ETA: 20s - loss: 0.5377 - accuracy: 0.7426\n18/94 [====&gt;.........................] - ETA: 19s - loss: 0.5388 - accuracy: 0.7407\n19/94 [=====&gt;........................] - ETA: 19s - loss: 0.5453 - accuracy: 0.7346\n20/94 [=====&gt;........................] - ETA: 19s - loss: 0.5474 - accuracy: 0.7323\n21/94 [=====&gt;........................] - ETA: 19s - loss: 0.5447 - accuracy: 0.7341\n22/94 [======&gt;.......................] - ETA: 19s - loss: 0.5416 - accuracy: 0.7367\n23/94 [======&gt;.......................] - ETA: 18s - loss: 0.5372 - accuracy: 0.7400\n24/94 [======&gt;.......................] - ETA: 18s - loss: 0.5376 - accuracy: 0.7396\n25/94 [======&gt;.......................] - ETA: 18s - loss: 0.5334 - accuracy: 0.7433\n26/94 [=======&gt;......................] - ETA: 18s - loss: 0.5279 - accuracy: 0.7484\n27/94 [=======&gt;......................] - ETA: 17s - loss: 0.5275 - accuracy: 0.7485\n28/94 [=======&gt;......................] - ETA: 17s - loss: 0.5281 - accuracy: 0.7470\n29/94 [========&gt;.....................] - ETA: 17s - loss: 0.5288 - accuracy: 0.7457\n30/94 [========&gt;.....................] - ETA: 17s - loss: 0.5288 - accuracy: 0.7451\n31/94 [========&gt;.....................] - ETA: 17s - loss: 0.5277 - accuracy: 0.7460\n32/94 [=========&gt;....................] - ETA: 16s - loss: 0.5271 - accuracy: 0.7461\n33/94 [=========&gt;....................] - ETA: 16s - loss: 0.5287 - accuracy: 0.7443\n34/94 [=========&gt;....................] - ETA: 16s - loss: 0.5287 - accuracy: 0.7439\n35/94 [==========&gt;...................] - ETA: 16s - loss: 0.5277 - accuracy: 0.7440\n36/94 [==========&gt;...................] - ETA: 15s - loss: 0.5267 - accuracy: 0.7448\n37/94 [==========&gt;...................] - ETA: 15s - loss: 0.5225 - accuracy: 0.7483\n38/94 [===========&gt;..................] - ETA: 15s - loss: 0.5226 - accuracy: 0.7473\n39/94 [===========&gt;..................] - ETA: 15s - loss: 0.5221 - accuracy: 0.7468\n40/94 [===========&gt;..................] - ETA: 14s - loss: 0.5217 - accuracy: 0.7469\n41/94 [============&gt;.................] - ETA: 14s - loss: 0.5215 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 14s - loss: 0.5213 - accuracy: 0.7460\n43/94 [============&gt;.................] - ETA: 14s - loss: 0.5211 - accuracy: 0.7452\n44/94 [=============&gt;................] - ETA: 13s - loss: 0.5202 - accuracy: 0.7453\n45/94 [=============&gt;................] - ETA: 13s - loss: 0.5188 - accuracy: 0.7458\n46/94 [=============&gt;................] - ETA: 13s - loss: 0.5202 - accuracy: 0.7437\n47/94 [==============&gt;...............] - ETA: 13s - loss: 0.5184 - accuracy: 0.7438\n48/94 [==============&gt;...............] - ETA: 12s - loss: 0.5173 - accuracy: 0.7444\n49/94 [==============&gt;...............] - ETA: 12s - loss: 0.5162 - accuracy: 0.7445\n50/94 [==============&gt;...............] - ETA: 12s - loss: 0.5167 - accuracy: 0.7437\n51/94 [===============&gt;..............] - ETA: 11s - loss: 0.5171 - accuracy: 0.7422\n52/94 [===============&gt;..............] - ETA: 11s - loss: 0.5150 - accuracy: 0.7436\n53/94 [===============&gt;..............] - ETA: 11s - loss: 0.5149 - accuracy: 0.7429\n54/94 [================&gt;.............] - ETA: 11s - loss: 0.5142 - accuracy: 0.7427\n55/94 [================&gt;.............] - ETA: 10s - loss: 0.5121 - accuracy: 0.7443\n56/94 [================&gt;.............] - ETA: 10s - loss: 0.5124 - accuracy: 0.7422\n57/94 [=================&gt;............] - ETA: 10s - loss: 0.5115 - accuracy: 0.7427\n58/94 [=================&gt;............] - ETA: 10s - loss: 0.5103 - accuracy: 0.7432\n59/94 [=================&gt;............] - ETA: 9s - loss: 0.5089 - accuracy: 0.7440 \n60/94 [==================&gt;...........] - ETA: 9s - loss: 0.5085 - accuracy: 0.7434\n61/94 [==================&gt;...........] - ETA: 9s - loss: 0.5076 - accuracy: 0.7439\n62/94 [==================&gt;...........] - ETA: 8s - loss: 0.5066 - accuracy: 0.7440\n63/94 [===================&gt;..........] - ETA: 8s - loss: 0.5067 - accuracy: 0.7421\n64/94 [===================&gt;..........] - ETA: 8s - loss: 0.5058 - accuracy: 0.7425\n65/94 [===================&gt;..........] - ETA: 8s - loss: 0.5044 - accuracy: 0.7436\n66/94 [====================&gt;.........] - ETA: 7s - loss: 0.5031 - accuracy: 0.7434\n67/94 [====================&gt;.........] - ETA: 7s - loss: 0.5012 - accuracy: 0.7447\n68/94 [====================&gt;.........] - ETA: 7s - loss: 0.5005 - accuracy: 0.7436\n69/94 [=====================&gt;........] - ETA: 7s - loss: 0.5011 - accuracy: 0.7412\n70/94 [=====================&gt;........] - ETA: 6s - loss: 0.4998 - accuracy: 0.7417\n71/94 [=====================&gt;........] - ETA: 6s - loss: 0.4972 - accuracy: 0.7435\n72/94 [=====================&gt;........] - ETA: 6s - loss: 0.4966 - accuracy: 0.7431\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.4949 - accuracy: 0.7443\n74/94 [======================&gt;.......] - ETA: 5s - loss: 0.4950 - accuracy: 0.7430\n75/94 [======================&gt;.......] - ETA: 5s - loss: 0.4929 - accuracy: 0.7444\n76/94 [=======================&gt;......] - ETA: 5s - loss: 0.4910 - accuracy: 0.7462\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.4900 - accuracy: 0.7465\n78/94 [=======================&gt;......] - ETA: 4s - loss: 0.4891 - accuracy: 0.7468\n79/94 [========================&gt;.....] - ETA: 4s - loss: 0.4888 - accuracy: 0.7458\n80/94 [========================&gt;.....] - ETA: 4s - loss: 0.4883 - accuracy: 0.7453\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.4880 - accuracy: 0.7454\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.4859 - accuracy: 0.7472\n83/94 [=========================&gt;....] - ETA: 3s - loss: 0.4845 - accuracy: 0.7482\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.4840 - accuracy: 0.7478\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.4835 - accuracy: 0.7473\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.4815 - accuracy: 0.7478\n87/94 [==========================&gt;...] - ETA: 2s - loss: 0.4813 - accuracy: 0.7476\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.4803 - accuracy: 0.7476\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.4788 - accuracy: 0.7488\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.4780 - accuracy: 0.7488\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.4774 - accuracy: 0.7491\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7486\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7498\n94/94 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.7501\n94/94 [==============================] - 29s 308ms/step - loss: 0.4745 - accuracy: 0.7501 - val_loss: 0.3965 - val_accuracy: 0.7998\nEpoch 3/3\n\n 1/94 [..............................] - ETA: 27s - loss: 0.3033 - accuracy: 0.8958\n 2/94 [..............................] - ETA: 28s - loss: 0.3199 - accuracy: 0.8750\n 3/94 [..............................] - ETA: 28s - loss: 0.3137 - accuracy: 0.8819\n 4/94 [&gt;.............................] - ETA: 30s - loss: 0.3038 - accuracy: 0.8698\n 5/94 [&gt;.............................] - ETA: 32s - loss: 0.3082 - accuracy: 0.8583\n 6/94 [&gt;.............................] - ETA: 31s - loss: 0.3238 - accuracy: 0.8438\n 7/94 [=&gt;............................] - ETA: 30s - loss: 0.3133 - accuracy: 0.8542\n 8/94 [=&gt;............................] - ETA: 29s - loss: 0.3192 - accuracy: 0.8438\n 9/94 [=&gt;............................] - ETA: 29s - loss: 0.3184 - accuracy: 0.8495\n10/94 [==&gt;...........................] - ETA: 30s - loss: 0.3140 - accuracy: 0.8542\n11/94 [==&gt;...........................] - ETA: 29s - loss: 0.3094 - accuracy: 0.8485\n12/94 [==&gt;...........................] - ETA: 29s - loss: 0.3068 - accuracy: 0.8490\n13/94 [===&gt;..........................] - ETA: 28s - loss: 0.3045 - accuracy: 0.8526\n14/94 [===&gt;..........................] - ETA: 28s - loss: 0.3027 - accuracy: 0.8542\n15/94 [===&gt;..........................] - ETA: 28s - loss: 0.2993 - accuracy: 0.8556\n16/94 [====&gt;.........................] - ETA: 27s - loss: 0.2950 - accuracy: 0.8607\n17/94 [====&gt;.........................] - ETA: 27s - loss: 0.2927 - accuracy: 0.8640\n18/94 [====&gt;.........................] - ETA: 27s - loss: 0.2916 - accuracy: 0.8657\n19/94 [=====&gt;........................] - ETA: 26s - loss: 0.2937 - accuracy: 0.8618\n20/94 [=====&gt;........................] - ETA: 26s - loss: 0.2921 - accuracy: 0.8635\n21/94 [=====&gt;........................] - ETA: 25s - loss: 0.2894 - accuracy: 0.8631\n22/94 [======&gt;.......................] - ETA: 25s - loss: 0.2855 - accuracy: 0.8655\n23/94 [======&gt;.......................] - ETA: 24s - loss: 0.2826 - accuracy: 0.8659\n24/94 [======&gt;.......................] - ETA: 24s - loss: 0.2806 - accuracy: 0.8672\n25/94 [======&gt;.......................] - ETA: 23s - loss: 0.2821 - accuracy: 0.8650\n26/94 [=======&gt;......................] - ETA: 23s - loss: 0.2804 - accuracy: 0.8646\n27/94 [=======&gt;......................] - ETA: 22s - loss: 0.2792 - accuracy: 0.8657\n28/94 [=======&gt;......................] - ETA: 22s - loss: 0.2804 - accuracy: 0.8631\n29/94 [========&gt;.....................] - ETA: 21s - loss: 0.2777 - accuracy: 0.8657\n30/94 [========&gt;.....................] - ETA: 21s - loss: 0.2797 - accuracy: 0.8632\n31/94 [========&gt;.....................] - ETA: 21s - loss: 0.2790 - accuracy: 0.8649\n32/94 [=========&gt;....................] - ETA: 20s - loss: 0.2754 - accuracy: 0.8672\n33/94 [=========&gt;....................] - ETA: 20s - loss: 0.2733 - accuracy: 0.8699\n34/94 [=========&gt;....................] - ETA: 19s - loss: 0.2713 - accuracy: 0.8713\n35/94 [==========&gt;...................] - ETA: 19s - loss: 0.2705 - accuracy: 0.8726\n36/94 [==========&gt;...................] - ETA: 19s - loss: 0.2685 - accuracy: 0.8738\n37/94 [==========&gt;...................] - ETA: 18s - loss: 0.2671 - accuracy: 0.8761\n38/94 [===========&gt;..................] - ETA: 18s - loss: 0.2663 - accuracy: 0.8750\n39/94 [===========&gt;..................] - ETA: 18s - loss: 0.2642 - accuracy: 0.8766\n40/94 [===========&gt;..................] - ETA: 17s - loss: 0.2629 - accuracy: 0.8781\n41/94 [============&gt;.................] - ETA: 17s - loss: 0.2622 - accuracy: 0.8775\n42/94 [============&gt;.................] - ETA: 17s - loss: 0.2600 - accuracy: 0.8800\n43/94 [============&gt;.................] - ETA: 16s - loss: 0.2621 - accuracy: 0.8789\n44/94 [=============&gt;................] - ETA: 16s - loss: 0.2628 - accuracy: 0.8788\n45/94 [=============&gt;................] - ETA: 16s - loss: 0.2613 - accuracy: 0.8796\n46/94 [=============&gt;................] - ETA: 15s - loss: 0.2600 - accuracy: 0.8786\n47/94 [==============&gt;...............] - ETA: 15s - loss: 0.2582 - accuracy: 0.8790\n48/94 [==============&gt;...............] - ETA: 15s - loss: 0.2571 - accuracy: 0.8793\n49/94 [==============&gt;...............] - ETA: 14s - loss: 0.2566 - accuracy: 0.8797\n50/94 [==============&gt;...............] - ETA: 14s - loss: 0.2568 - accuracy: 0.8788\n51/94 [===============&gt;..............] - ETA: 14s - loss: 0.2557 - accuracy: 0.8795\n52/94 [===============&gt;..............] - ETA: 13s - loss: 0.2562 - accuracy: 0.8786\n53/94 [===============&gt;..............] - ETA: 13s - loss: 0.2548 - accuracy: 0.8801\n54/94 [================&gt;.............] - ETA: 13s - loss: 0.2524 - accuracy: 0.8819\n55/94 [================&gt;.............] - ETA: 12s - loss: 0.2509 - accuracy: 0.8833\n56/94 [================&gt;.............] - ETA: 12s - loss: 0.2505 - accuracy: 0.8847\n57/94 [=================&gt;............] - ETA: 11s - loss: 0.2518 - accuracy: 0.8834\n58/94 [=================&gt;............] - ETA: 11s - loss: 0.2502 - accuracy: 0.8840\n59/94 [=================&gt;............] - ETA: 11s - loss: 0.2495 - accuracy: 0.8845\n60/94 [==================&gt;...........] - ETA: 10s - loss: 0.2482 - accuracy: 0.8851\n61/94 [==================&gt;...........] - ETA: 10s - loss: 0.2473 - accuracy: 0.8852\n62/94 [==================&gt;...........] - ETA: 10s - loss: 0.2463 - accuracy: 0.8861\n63/94 [===================&gt;..........] - ETA: 9s - loss: 0.2461 - accuracy: 0.8856 \n64/94 [===================&gt;..........] - ETA: 9s - loss: 0.2446 - accuracy: 0.8870\n65/94 [===================&gt;..........] - ETA: 9s - loss: 0.2431 - accuracy: 0.8878\n66/94 [====================&gt;.........] - ETA: 8s - loss: 0.2420 - accuracy: 0.8889\n67/94 [====================&gt;.........] - ETA: 8s - loss: 0.2420 - accuracy: 0.8887\n68/94 [====================&gt;.........] - ETA: 8s - loss: 0.2401 - accuracy: 0.8900\n69/94 [=====================&gt;........] - ETA: 7s - loss: 0.2406 - accuracy: 0.8895\n70/94 [=====================&gt;........] - ETA: 7s - loss: 0.2386 - accuracy: 0.8905\n71/94 [=====================&gt;........] - ETA: 7s - loss: 0.2376 - accuracy: 0.8911\n72/94 [=====================&gt;........] - ETA: 6s - loss: 0.2365 - accuracy: 0.8918\n73/94 [======================&gt;.......] - ETA: 6s - loss: 0.2360 - accuracy: 0.8918\n74/94 [======================&gt;.......] - ETA: 6s - loss: 0.2354 - accuracy: 0.8922\n75/94 [======================&gt;.......] - ETA: 6s - loss: 0.2341 - accuracy: 0.8928\n76/94 [=======================&gt;......] - ETA: 5s - loss: 0.2318 - accuracy: 0.8942\n77/94 [=======================&gt;......] - ETA: 5s - loss: 0.2297 - accuracy: 0.8956\n78/94 [=======================&gt;......] - ETA: 5s - loss: 0.2284 - accuracy: 0.8958\n79/94 [========================&gt;.....] - ETA: 4s - loss: 0.2277 - accuracy: 0.8961\n80/94 [========================&gt;.....] - ETA: 4s - loss: 0.2269 - accuracy: 0.8966\n81/94 [========================&gt;.....] - ETA: 4s - loss: 0.2265 - accuracy: 0.8963\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.2257 - accuracy: 0.8966\n83/94 [=========================&gt;....] - ETA: 3s - loss: 0.2259 - accuracy: 0.8966\n84/94 [=========================&gt;....] - ETA: 3s - loss: 0.2249 - accuracy: 0.8971\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.2235 - accuracy: 0.8978\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.2222 - accuracy: 0.8985\n87/94 [==========================&gt;...] - ETA: 2s - loss: 0.2208 - accuracy: 0.8994\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.2199 - accuracy: 0.9001\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.2198 - accuracy: 0.9000\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.2200 - accuracy: 0.9002\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9011\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9010\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9017\n94/94 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9017\n94/94 [==============================] - 30s 318ms/step - loss: 0.2173 - accuracy: 0.9017 - val_loss: 0.3078 - val_accuracy: 0.8811\n&lt;keras.src.callbacks.History object at 0x0000024435E3FE10&gt;\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\n\ny_pred_probs = model.predict(X_test)\n\n\n 1/35 [..............................] - ETA: 7s\n 4/35 [==&gt;...........................] - ETA: 0s\n 7/35 [=====&gt;........................] - ETA: 0s\n10/35 [=======&gt;......................] - ETA: 0s\n13/35 [==========&gt;...................] - ETA: 0s\n15/35 [===========&gt;..................] - ETA: 0s\n18/35 [==============&gt;...............] - ETA: 0s\n21/35 [=================&gt;............] - ETA: 0s\n24/35 [===================&gt;..........] - ETA: 0s\n27/35 [======================&gt;.......] - ETA: 0s\n30/35 [========================&gt;.....] - ETA: 0s\n33/35 [===========================&gt;..] - ETA: 0s\n35/35 [==============================] - 1s 21ms/step\n\ny_pred = (model.predict(X_test) &gt; 0.5).astype(\"int32\")\n\n\n 1/35 [..............................] - ETA: 2s\n 4/35 [==&gt;...........................] - ETA: 0s\n 7/35 [=====&gt;........................] - ETA: 0s\n11/35 [========&gt;.....................] - ETA: 0s\n14/35 [===========&gt;..................] - ETA: 0s\n17/35 [=============&gt;................] - ETA: 0s\n20/35 [================&gt;.............] - ETA: 0s\n23/35 [==================&gt;...........] - ETA: 0s\n26/35 [=====================&gt;........] - ETA: 0s\n28/35 [=======================&gt;......] - ETA: 0s\n32/35 [==========================&gt;...] - ETA: 0s\n35/35 [==============================] - ETA: 0s\n35/35 [==============================] - 1s 21ms/step\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy}\")\n\nTest Accuracy: 0.8811438784629133\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\n\nConfusion Matrix:\n\nprint(conf_matrix)\n\n[[812  21]\n [112 174]]\n\n\nDas neuronale Netzwerk sagt das Train-Sample zwar perfekt vorher, hat jedoch vergleichsweise gro√üe Schwierigkeiten beim Test-Sample.\n\n\n\nDurch die explorative Datenanalyse war es m√∂glich, einige relevante Charakteristika herauszuarbeiten, die Hate-Speech-Tweets klar von anderen Tweets abgrenzen. Hate Speech enth√§lt n√§mlich einen gro√üen Anteil an Beleidigungen und Schimpfw√∂rtern sowie negativen Sentimenten. Diese Erkenntnisse waren f√ºr die Modellierung hilfreich, da es gelang, Features basierend auf der EDA zu generieren, die von hoher Relevanz f√ºr die Performance des Modells waren. Das Ziel der Modellierung war es, ein Modell zu trainieren, das Hate Speech in Tweets m√∂glichst akkurat erkennt. Durch den kombinierten Ansatz aus Training und Deep Learning wurde dieses Ziel mit Erfolg erreicht, auch wenn die Deep Learning Modelle vergleichsweise schlecht abschnitten."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#einleitung",
    "href": "posts/Hatespeech_Twitter.html#einleitung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel dieses Posts ist es, Hate Speech auf Twitter zu klassifizieren. Hass im Internet ist nach wie vor ein gro√ües gesellschaftliches Problem, weshalb es sich lohnt, genauer zu untersuchen, was diesen Hass ausmacht und wie man ihn zuverl√§ssig und automatisiert erkennen kann. Hierf√ºr liegt ein Datensatz vor, der eine Auswahl englischer als Hate Speech oder nicht Hate Speech markierter Tweets enth√§lt. Die Analyse dieser Daten l√§sst sich in zwei Teile gliedern: Zun√§chst werden Methoden der explorativen Datenanalyse angewandt, um Muster und Auff√§lligkeiten in den Tweets zu identifizieren. Anschlie√üend werden die gewonnenen Erkenntnisse genutzt, um sowohl Shallow-Learning- als auch Deep-Learning-Algorithmen darauf zu trainieren, Tweets korrekterweise als Hate Speech einzuordnen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#vorbereitung",
    "href": "posts/Hatespeech_Twitter.html#vorbereitung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "library(tidymodels)\nlibrary(textrecipes)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidytext)\nlibrary(syuzhet)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(stringr)\nlibrary(readr)\nlibrary(vip)\n\n\n\n\nBei den Daten handelt es sich um eine Auswahl englischer Tweets, die bereits auf Hate Speech untersucht wurden und sich daher gut f√ºr das Training von Modellen zur Erkennung von Hassrede eignen.\n\nd_hate &lt;- read_csv(\"d_hate.csv\")"
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#explorative-datenanalyse",
    "href": "posts/Hatespeech_Twitter.html#explorative-datenanalyse",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel ist es, auf Grundlage der Tweets einige n√ºtzliche Features zu generieren, die sich als Pr√§diktor f√ºr die AV (Hatespeech oder nicht) eignen. Hierf√ºr m√ºssen zun√§chst einige Charakteristika von Tweets, die als Hate Speech gelten, herausgearbeitet werden. Die Methoden und der Code orientieren sich stark an dem Vorgehen, das in Julia Silges und David Robinsons Buch ‚ÄúText Mining with R‚Äù (https://www.tidytextmining.com/) beschrieben wird. F√ºr die nachfolgenden Visualisierungen wird eine mit Hilfe der Seite https://davidmathlogic.com/colorblind eigens erstellte Farbpalette verwendet, die gew√§hrleistet, dass keine Art der Farbenblindheit die Lesbarkeit der Diagramme beeintr√§chtigt.\n\nUriah_Flint &lt;- c(\"#8175AA\", \"#6FB899\", \"#3AA2C3\", \"#8BD4F9\", \"#DDCC77\", \"#CC6677\", \"#882255\")\n\n\n\nUm eine sinnvolle Analyse durchzuf√ºhren, m√ºssen noch einige Datenvorverarbeitungsschritte durchlaufen werden. Diese beinhalten die Tokenisierung, das Entfernen von Stopwords und das Bereinigen der Tweets, die Links oder √§hnliche Elemente enthalten.\n\nd_hate_clean &lt;- d_hate %&gt;%\n  mutate(tweet = str_remove_all(tweet, pattern = 'RT\\\\s*|http[s]?://\\\\S+|\\\\d+'))\n\nset.seed(123)\ntrain_test_split &lt;- initial_split(d_hate_clean, prop = .8, strata = class)\nd_train &lt;- training(train_test_split)\nd_test &lt;- testing(train_test_split)\n\n\n\n\ntweets_token &lt;- d_train %&gt;%\n  unnest_tokens(word, tweet)\n\n\n\n\n\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\n\ntweets_token &lt;- tweets_token %&gt;%\n  anti_join(stopwords_en)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nsenti &lt;- get_sentiments(\"afinn\") %&gt;% \n  mutate(neg_pos = case_when(value &gt; 0 ~ \"pos\",\n                             TRUE ~ \"neg\"))\n\ntweets_senti &lt;- tweets_token %&gt;%\ninner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nUm sich einen ersten √úberblick √ºber die Daten zu verschaffen, ist es sinnvoll, zun√§chst einmal den Anteil der als Hate Speech markierten Tweets zu √ºberpr√ºfen.\n\ntweets_token %&gt;% \n  summarise(`Anteil Hate Speech` = mean(class == \"hate speech\")) %&gt;% \n  round(2)\n\n# A tibble: 1 √ó 1\n  `Anteil Hate Speech`\n                 &lt;dbl&gt;\n1                 0.25\n\n\n\nclass_totals &lt;- tweets_token %&gt;%\n  count(class, name = \"class_total\")\n\nggplot(class_totals, aes(x = \"\", y = class_total, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil an Hate Speech\",\n       x = NULL,\n       y = NULL,\n       fill = \"Klasse\") +\n  geom_text(aes(label = class_total), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nDer Anteil der Hate Speech in diesem Datensatz betr√§gt 25 Prozent. Die Tweets anderer Kategorien sind also deutlich in der Mehrheit.\n\n\n\nEinen weiteren interessanten Einblick gew√§hren die Worth√§ufigkeiten. Durch die Visualisierung der am meisten verwendeten W√∂rter und Wortpaare ist es bereits m√∂glich, einen Einblick in das Vokabular zu erhalten und dieses unter den Klassen zu vergleichen.\n\ntweets_count_senti &lt;- tweets_senti %&gt;%\n  group_by(class) %&gt;% \n  count(class, word, sort = TRUE) %&gt;% \n  slice_head(n = 10)\n\n# Zusammenf√ºhren von \"word_counts\" und \"class_totals\" nach der Klasse\nword_counts &lt;- left_join(tweets_count_senti, class_totals, by = \"class\")\n\n# Berechnung der gewichteten H√§ufigkeit\nword_counts &lt;- word_counts %&gt;%\n  mutate(weighted_frequency = n / class_total)\n\n# Visualisierung der gewichteten H√§ufigkeiten\nggplot(word_counts, aes(x = reorder(word, weighted_frequency), y = weighted_frequency, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip() +\n  labs(title = \"Gewichtete H√§ufigkeiten der W√∂rter in Abh√§ngigkeit von der Klasse\",\n       x = \"Wort\",\n       y = \"Gewichtete H√§ufigkeit\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nBeim Vergleich der h√§ufigsten W√∂rter f√§llt direkt auf, dass Beleidigungen und Schimpfw√∂rter charakteristisch f√ºr Hate Speech sind, da die Liste der zehn h√§ufigsten W√∂rter fast nur aus solchen Eintr√§gen besteht. Das Vokabular der anderen Kategorie ist im Vergleich dazu √ºberaus harmlos. Diese Harmlosigkeit wird durch das h√§ufigste Wort ‚Äúlol‚Äù noch auf die Spitze getrieben. Interessant ist jedoch auch, dass sich das Wort ‚Äúhate‚Äù in dieser Liste wiederfindet. Hier w√§re es interessant, im weiteren Verlauf der Analyse den Kontext in Erfahrung zu bringen. Auf der anderen Seite ist ‚Äúhate‚Äù jedoch ein sehr g√§ngiges Wort und dient zur Beschreibung normaler Gef√ºhlszust√§nde, ohne direkt Hass zu verbreiten.\n\ntweets_bigram &lt;- \n  d_train %&gt;%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ntweets_bigram &lt;- tweets_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntweets_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  group_by(class) %&gt;% \n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram, fill = class) ) +\n  facet_wrap(~class, scales = \"free_y\") +\n  geom_col() +\n  labs(title = \"Bigramme nach H√§ufigkeit\",\n       x = \"H√§ufigkeit\",\n       y = \"Bigramm\") +\n  scale_fill_manual(values = Uriah_Flint) +\n  theme_light()\n\n\n\n\nDie Analyse der h√§ufigsten Wortpaare deckt sich mit der Analyse der h√§ufigsten W√∂rter. Sie bringt insofern neue Erkenntnisse, als deutlich wird, dass sich der Hass haupts√§chlich gegen ethnische und sexuelle Minderheiten richtet. Dies wird anhand von Begriffen wie ‚Äúwhite trash‚Äù und ‚Äúfucking faggot‚Äù deutlich. Bemerkenswert ist ebenfalls, dass es sich keinesfalls haupts√§chlich um Hass gegen Schwarze handelt, sondern genauso auch Menschen mit heller Hautfarbe ethnisch beleidigt werden.\n\n\n\nIm Folgenden werden alle Wortpaare, die h√§ufiger als sechs Mal vorkommen, visualisiert. Hierdurch werden die Kontexte der W√∂rter deutlicher und die Beziehungen k√∂nnen uns Aufschluss dar√ºber geben, in welchem Zusammenhang ‚Äúhate‚Äù verwendet wird.\n\ntweets_bigram_count &lt;- tweets_bigram %&gt;% \n   count(word1, word2, sort = TRUE)\n\nvisualize_bigrams &lt;- function(bigrams) {\n  set.seed(2016)\n  a &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n  \n  bigrams %&gt;%\n    graph_from_data_frame() %&gt;%\n    ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +\n    geom_node_point(color = \"#6FB899\", size = 5) +\n    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n    theme_void()\n}\n\ntweets_bigram_count %&gt;%\n  filter(n &gt; 6,\n         !str_detect(word1, \"\\\\d\"),\n         !str_detect(word2, \"\\\\d\")) %&gt;%\n  visualize_bigrams()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nZum Kontext des Wortes ‚Äúhate‚Äù erhalten wir hier keine neuen Hinweise. Jedoch wird klar ersichtlich, in welchen Kombinationen Schimpfw√∂rter verwendet werden. Au√üerdem werden Ambiguit√§ten deutlich, da W√∂rter wie ‚Äútrash‚Äù und ‚Äúcolored‚Äù sowohl als rassistische Beleidigung als auch als Beschreibung von Alltagsgegenst√§nden auftauchen.\n\n\n\nZweck der Sentimentanalyse ist es, herauszufinden, ob die Sentimentauspr√§gungen die beiden Klassen klar voneinander abgrenzen.\n\n# Z√§hlen der negativen und positiven Sentimente\ntweets_senti2 &lt;- tweets_senti %&gt;% \n  group_by(class) %&gt;% \n  count(neg_pos, name = \"count\")\n\n# Visualisierung der Sentimentantanteile nach Klasse\nggplot(tweets_senti2, aes(x = \"\", y = count, fill = neg_pos)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sentimentanteile nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Sentiment\") +\n  facet_wrap(~ class) +\n  geom_text(aes(label = count), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nIn obigem Diagramm wird ersichtlich, dass hasslastige Tweets √ºberwiegend negativ sind, w√§hrend sich die Sentimente der anderen Klasse in der Waage halten. Das Sentiment ist also ein entscheidender Faktor bei der Klassifizierung von Hate Speech und sollte beim Training des Modells ber√ºcksichtigt werden.\n\n\n\nDie Themenanalyse soll Aufschluss dar√ºber geben, ob es bestimmte Themengebiete gibt, die charakteristisch f√ºr Hate Speech sind.\n\ntweets_token_counts_hate &lt;- tweets_token %&gt;%\n  filter(class == \"hate speech\") %&gt;% \n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 19) %&gt;% \n  select(word)\n\ntweets_dtm_hate &lt;- DocumentTermMatrix(tweets_token_counts_hate)\ntweets_dtm_hate\n\n\ntweets_lda_hate &lt;- LDA(tweets_dtm_hate, k = 4, control = list(seed = 42))\n\ntweets_themen_hate &lt;- tidy(tweets_lda_hate, matrix = \"beta\")\n\ntweets_themen_hate &lt;- tweets_themen_hate %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 7) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntweets_themen_hate %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(title = \"Themen von Hate Speech\") +\n  theme_minimal() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nObwohl sich die Themen nicht eindeutig voneinander abgrenzen, sind dennoch schwache Muster erkennbar. Thema Eins scheint sich vor allem aus allgemeinen Obsz√∂nit√§ten zusammenzusetzen, w√§hrend das zweite Thema aus Beleidigungen gegen Schwule und Schwarze und etwas h√§rteren W√∂rtern wie ‚Äúkill‚Äù und ‚Äúshit‚Äù besteht. In Thema Drei und Vier treten Beleidigungen gegen Frauen sowie die LGBTQ-Community in den Vordergrund. Viel wichtiger als diese kleinen Unterschiede ist jedoch das gro√üe Bild der Themen, welches wie schon bei der Analyse der Worth√§ufigkeiten festgestellt, haupts√§chlich aus ethnischen und sexuellen Beleidigungen und Schimpfw√∂rtern besteht.\n\n\n\nSchimpfw√∂rter scheinen eine gro√üe Rolle bei Hate Speech zu spielen. Deshalb erachte ich es als sinnvoll, Schimpfw√∂rter als Feature in das sp√§tere Rezept mit aufzunehmen. Hierzu verwende ich diese Liste, welche ich um ein paar Eintr√§ge (rassistische Beleidigungen) erg√§nzt habe: https://www.insult.wiki/list-of-insults.\n\ninsults &lt;- read.csv(\"insults.csv\")\n\ntweets_token %&gt;%\n  group_by(class) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(insults, by = \"word\") %&gt;% \n  mutate(insult = case_when(is.na(value) == TRUE ~ \"Nein\",\n                            TRUE ~ \"Ja\")) %&gt;% \n  select(-value) %&gt;% \nggplot(aes(x = \"\", y = n, fill = insult)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil der Beleidigungen nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Beleidigung\") +\n  facet_wrap(~ class, scales = \"free_y\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nTats√§chlich ist der Anteil der Beleidigungen in Hate Speech Tweets h√∂her, jedoch f√§llt er deutlich geringer aus als erwartet.\n\n\n\nDie √úberlegung, dass Hate Speech feindselige Emojis enth√§lt, ist sehr plausibel. Um aggressive Emojis zu kennzeichnen, verwende ich ein von mir erstelltes Lexikon, das solche Emojis enth√§lt und schreibe eine Funktion, die z√§hlt, wieviele feindselige Emojis in einem Tweet vorkommen. Der Totenkopf ist nicht in der Liste der feindseligen Emojis enthalten, da dieser haupts√§chlich als Synonym oder Steigerung des Lach-Emojis verwendet wird (engl.: ‚ÄúThat‚Äôs too funny. I‚Äôm dead!‚Äù).\n\nhostile_emojis &lt;- read.csv(\"hostile_emojis.csv\")\n\n\ncount_hostile_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor f√ºr die Z√§hlungen\n  counts &lt;- numeric(length(hostile_emojis$emoji))\n\n  # Iteriere √ºber jedes Emoji und z√§hle die √úbereinstimmungen im Text\n  for (i in seq_along(hostile_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, hostile_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der √úbereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"üóë\", \"bogen\", \"üò†\", \"üëπ\", \"üí©\", \"baby\", \"und\", \"üÜó\")\ncount_hostile_emojis(dummy)\n\n[1] 3\n\n\n\nd_train %&gt;% \n  mutate(hostile_emojis_n = map_int(tweet, count_hostile_emojis)) %&gt;% \n  summarise(`Feindselige Emojis` = mean(hostile_emojis_n == 1))\n\n# A tibble: 1 √ó 1\n  `Feindselige Emojis`\n                 &lt;dbl&gt;\n1                    0\n\n\nDie Vermutung, dass Hate Speech feindselige Emojis enth√§lt, stellt sich in diesem Fall als falsch heraus. Da es keinen einzigen Emoji dieser Art gibt, wird dieser Ansatz f√ºr die Modellierung verworfen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#modellierung",
    "href": "posts/Hatespeech_Twitter.html#modellierung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "In der Modellierung ist es nun das Ziel, einen Algorithmus darauf zu trainieren, m√∂glichst pr√§zise Hate Speech vorherzusagen. Der Algorithmus meiner Wahl ist der XGBoost. Zun√§chst werden jedoch noch Rezepte formuliert, die die Erkenntnisse aus der Analyse nun in n√ºtzliche Features umwandeln.\n\n\nRezept Eins enth√§lt Schimpfw√∂rter, Sentimentwerte und die Themenanalyse. Au√üerdem werden noch die √ºblichen Textverarbeitungsschritte durchgef√ºhrt sowie ein Tokenfilter angewandt.\n\nrec1 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;%\n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_lda(tweet, num_topics = 6)\n\nRezept Zwei enth√§lt statt der Themenanalyse die Tf-idf-Ma√üe.\n\nrec2 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_tfidf(tweet)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n# A tibble: 4,474 √ó 10\n      id class      insult senti lda_tweet_1 lda_tweet_2 lda_tweet_3 lda_tweet_4\n   &lt;dbl&gt; &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1    85 hate spee‚Ä¶      1 -0.25       0            0           0            0  \n 2    90 hate spee‚Ä¶      1  1          0.1          0.3         0            0.6\n 3   186 hate spee‚Ä¶      1 -1          0.433        0.4         0            0.1\n 4   204 hate spee‚Ä¶      1 -0.75       0            0           0.3          0.2\n 5   206 hate spee‚Ä¶      2 -0.75       0            0.3         0.5          0  \n 6   221 hate spee‚Ä¶      0  1.4        0            0           0            0.2\n 7   263 hate spee‚Ä¶      1 -1.75       0            0           0.1          0.4\n 8   317 hate spee‚Ä¶      1 -0.55       0            0           0.5          0  \n 9   320 hate spee‚Ä¶      0 -0.35       0.1          0.1         0            0.2\n10   354 hate spee‚Ä¶      1 -0.75       0.25         0.15        0.05         0.5\n# ‚Ñπ 4,464 more rows\n# ‚Ñπ 2 more variables: lda_tweet_5 &lt;dbl&gt;, lda_tweet_6 &lt;dbl&gt;\n\n\n\nbaked2 &lt;- rec2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n# A tibble: 4,474 √ó 44\n      id class     insult senti tfidf_tweet_amp tfidf_tweet_ass tfidf_tweet_back\n   &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1    85 hate spe‚Ä¶      1 -0.25               0               0                0\n 2    90 hate spe‚Ä¶      1  1                  0               0                0\n 3   186 hate spe‚Ä¶      1 -1                  0               0                0\n 4   204 hate spe‚Ä¶      1 -0.75               0               0                0\n 5   206 hate spe‚Ä¶      2 -0.75               0               0                0\n 6   221 hate spe‚Ä¶      0  1.4                0               0                0\n 7   263 hate spe‚Ä¶      1 -1.75               0               0                0\n 8   317 hate spe‚Ä¶      1 -0.55               0               0                0\n 9   320 hate spe‚Ä¶      0 -0.35               0               0                0\n10   354 hate spe‚Ä¶      1 -0.75               0               0                0\n# ‚Ñπ 4,464 more rows\n# ‚Ñπ 37 more variables: tfidf_tweet_bird &lt;dbl&gt;, tfidf_tweet_birds &lt;dbl&gt;,\n#   tfidf_tweet_bitch &lt;dbl&gt;, tfidf_tweet_can &lt;dbl&gt;, tfidf_tweet_charlie &lt;dbl&gt;,\n#   tfidf_tweet_colored &lt;dbl&gt;, tfidf_tweet_day &lt;dbl&gt;, tfidf_tweet_faggot &lt;dbl&gt;,\n#   tfidf_tweet_fuck &lt;dbl&gt;, tfidf_tweet_get &lt;dbl&gt;, tfidf_tweet_ghetto &lt;dbl&gt;,\n#   tfidf_tweet_go &lt;dbl&gt;, tfidf_tweet_good &lt;dbl&gt;, tfidf_tweet_got &lt;dbl&gt;,\n#   tfidf_tweet_hate &lt;dbl&gt;, tfidf_tweet_just &lt;dbl&gt;, tfidf_tweet_know &lt;dbl&gt;, ‚Ä¶\n\n\n\n\n\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n\nDas Modell wird getuned. Hierf√ºr wird zweifache Kreuzvalidierung mit einer Wiederholung verwendet. Der geringe Performance-Zuwachs durch intensiveres Tuning mit mehr Folds und Wiederholungen w√ºrde in diesem Fall nicht die h√∂here Rechenzeit rechtfertigen.\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n\ni 1 of 2 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n‚úî 1 of 2 tuning:     rec1_xgb (1m 13.7s)\n\n\ni 2 of 2 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n‚úî 2 of 2 tuning:     rec2_xgb (1m 25s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 28 √ó 9\n   wflow_id .config         preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.904     2 0.00857\n 2 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.881     2 0.0115 \n 3 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.880     2 0.00246\n 4 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.868     2 0.0196 \n 5 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.866     2 0.00112\n 6 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.855     2 0.00983\n 7 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.854     2 0.0115 \n 8 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.853     2 0.0140 \n 9 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ roc_auc binary     0.852     2 0.0167 \n10 rec2_xgb Preprocessor1_‚Ä¶ recipe  boos‚Ä¶ accura‚Ä¶ binary     0.848     2 0.00201\n# ‚Ñπ 18 more rows\n\n\nRezept Zwei hat besser abgeschnitten als Rezept Eins. W√§hlen wir nun das beste Modell aus und fitten es:\n\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec2_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec2_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\nDie Analyse der wichtigsten Pr√§diktoren deckt sich mit den Erkenntnissen aus der EDA. Die mit Abstand wichtigsten Features sind die Beleidigungen und Sentimentwerte, w√§hrend die Tf-idf-Ma√üe von Beleidigungen ebenfalls viel zur Prediction beitragen.\n\nwf_preds &lt;-\n  collect_predictions(model_set)\n\nwf_preds %&gt;%\n  group_by(wflow_id) %&gt;% \n  roc_curve(truth = class, `.pred_hate speech`) %&gt;% \n  autoplot()\n\n\n\n\nDie Performance im Train-Sample f√§llt sehr gut aus, da die Vorhersagen mit einer Genauigkeit von rund 90 Prozent sehr pr√§zise sind."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#vorhersagen",
    "href": "posts/Hatespeech_Twitter.html#vorhersagen",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "preds &lt;- predict(fit_final, d_test)\npreds\n\n# A tibble: 1,119 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 other      \n 2 other      \n 3 hate speech\n 4 other      \n 5 hate speech\n 6 other      \n 7 other      \n 8 other      \n 9 other      \n10 other      \n# ‚Ñπ 1,109 more rows\n\n\n\n\n\nd_test1 &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(class = as.factor(class))\nd_test1\n\n# A tibble: 1,119 √ó 4\n      id tweet                                                 class .pred_class\n   &lt;dbl&gt; &lt;chr&gt;                                                 &lt;fct&gt; &lt;fct&gt;      \n 1    66 \"\\\"@AllAboutManFeet:  woof woof and hot soles\"        other other      \n 2    70 \"\\\"@ArizonasFinest: Why the eggplant emoji doe?\\\"y h‚Ä¶ other other      \n 3   111 \"\\\"@DevilGrimz: @VigxRArts you're fucking gay, black‚Ä¶ hate‚Ä¶ hate speech\n 4   116 \"\\\"@DomWorldPeace: Baseball season for the win. #Yan‚Ä¶ other other      \n 5   151 \"\\\"@JReebo: Who wants to get there nose in these bad‚Ä¶ other hate speech\n 6   192 \"\\\"@Mesha_nojas: @_Vontethekidd &#;&#;&#;&#;&#;\\\" I ‚Ä¶ other other      \n 7   245 \"\\\"@Stephenwildboy: Chilling &#;&#;  sexy scally lad\" other other      \n 8   251 \"\\\"@TEE_JONEZ: @KingCuh @stanleys I'm not mad cuh ha‚Ä¶ other other      \n 9   318 \"\\\"@ayyee_ceee_: One mans trash &#; is another mans ‚Ä¶ other other      \n10   341 \"\\\"@hxhassan: Jihadi patron Hajjaj al-Ajmi is arrest‚Ä¶ other other      \n# ‚Ñπ 1,109 more rows\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test1,\n           truth = class,\n           estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.883\n2 f_meas   binary         0.744\n\n\nAuch im Test-Sample bew√§hrt sich das Modell mit einer sehr hohen Genauigkeit."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#klassifikation-mit-transformer",
    "href": "posts/Hatespeech_Twitter.html#klassifikation-mit-transformer",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ein weiterer Ansatz zur Klassifikation von Hate Speech ist es, kein eigenes Modell zu trainieren, sondern Zero-Shot-Learning anzuwenden. Das ergibt nat√ºrlich am meisten Sinn mit einem sehr fortgeschrittenen und komplexen Transformer-Modell, das bereits auf die Erkennung von Hate Speech trainiert wurde. Im Folgenden wird daher das Modell roberta-hate-speech-dynabench-r4-target von Facebook, welches auf Huggingface.co verf√ºgbar ist, um die Tweets nach Hate Speech zu klassifizieren. Hierzu wird der Befehl pipeline aus der transformers-Library von Huggingface genutzt, um das Modell zu laden und auf das Test-Sample anzuwenden.\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.2.3\n\n\n\nuse_virtualenv(\"C:/Users/rapha/venv\")\n\n\nfrom transformers import pipeline\nimport tensorflow as tf\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n\n\n\ntweets &lt;- d_test$tweet\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult &lt;- py$results\nlabels &lt;- lapply(result, function(element) element$label)\ntweets_hate &lt;- cbind(d_test, pred = unlist(labels))\ntweets_hate &lt;- tweets_hate %&gt;% \n  mutate(class = as.factor(class),\n         pred = case_when(pred == \"hate\" ~ \"hate speech\",\n            pred == \"nothate\" ~ \"other\"),\n         pred = as.factor(pred))\n\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(tweets_hate,\n           truth = class,\n           estimate = pred)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.882\n2 f_meas   binary         0.797\n\n\nDie Performance des Modells ist objektiv gesehen gut, verglichen mit dem XGBoost mit einer Minute Trainingszeit f√§llt sie jedoch mager aus."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#neuronales-netzwerk",
    "href": "posts/Hatespeech_Twitter.html#neuronales-netzwerk",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Bisher wurde Hate Speech sowohl mit Hilfe eines auf den konkreten Daten trainierten Shallow-Learner als auch mit Hilfe eines vortrainierten Transformers klassifiziert. Im letzten Schritt dieses Posts sollen die St√§rken dieser beiden Ans√§tze kombiniert werden, indem ein Deep-Learning-Algorithums, genauer gesagt ein Neuronales Netzwerk, auf den vorliegenden Daten trainiert wird. Das neuronale Netz verwendet ein vortrainiertes Wort-Einbettungsmodell mit 50 Dimensionen, das f√ºr die deutsche Sprache optimiert ist. Dieses Embedding-Modell erm√∂glicht es dem Netzwerk, semantische Repr√§sentationen der W√∂rter zu erlernen. Das Netzwerk besteht aus einer Eingabeschicht, die das Embedding-Modell enth√§lt, gefolgt von einer vollst√§ndig verbundenen Schicht mit 32 Neuronen und einer Sigmoid-Aktivierungsfunktion. Weiterhin gibt es eine Schicht mit 24 Neuronen und einer ReLU-Aktivierung. Die Ausgabeschicht besteht aus einem einzelnen Neuron f√ºr bin√§re Klassifikation. Das Netzwerk wird mit dem Adam-Optimizer kompiliert und die bin√§re Kreuzentropie wird als Verlustfunktion verwendet. Die Accuracy wird als Metrik √ºberwacht. Das Training erfolgt √ºber 3 Epochen mit einer Batch-Gr√∂√üe von 48, wobei die Validierung anhand des Test-Samples durchgef√ºhrt wird.\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n\nd_train = r.d_train\nd_test = r.d_test\n\nX_train = d_train[\"tweet\"].values\nX_test = d_test[\"tweet\"].values\n\n\nd_train[\"y\"] = d_train[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_train = d_train.loc[:, \"y\"].values\n\nd_test[\"y\"] = d_test[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_test = d_test.loc[:, \"y\"].values\n\n\nembedding = \"https://tfhub.dev/google/nnlm-de-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\ntf.random.set_seed(42)\n\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(32, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(24, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n\n\nmodel.fit(X_train, y_train,\nepochs=3,\nbatch_size=48,\nvalidation_data=(X_test, y_test),\nverbose = 1)\n\nEpoch 1/3\n\n 1/94 [..............................] - ETA: 3:06 - loss: 0.6014 - accuracy: 0.7500\n 2/94 [..............................] - ETA: 1:45 - loss: 0.5919 - accuracy: 0.7604\n 3/94 [..............................] - ETA: 1:05 - loss: 0.5953 - accuracy: 0.7500\n 4/94 [&gt;.............................] - ETA: 51s - loss: 0.6090 - accuracy: 0.7240 \n 5/94 [&gt;.............................] - ETA: 46s - loss: 0.6009 - accuracy: 0.7333\n 6/94 [&gt;.............................] - ETA: 42s - loss: 0.5789 - accuracy: 0.7604\n 7/94 [=&gt;............................] - ETA: 38s - loss: 0.5799 - accuracy: 0.7560\n 8/94 [=&gt;............................] - ETA: 37s - loss: 0.5771 - accuracy: 0.7578\n 9/94 [=&gt;............................] - ETA: 35s - loss: 0.5706 - accuracy: 0.7639\n10/94 [==&gt;...........................] - ETA: 33s - loss: 0.5739 - accuracy: 0.7583\n11/94 [==&gt;...........................] - ETA: 31s - loss: 0.5629 - accuracy: 0.7689\n12/94 [==&gt;...........................] - ETA: 30s - loss: 0.5633 - accuracy: 0.7674\n13/94 [===&gt;..........................] - ETA: 29s - loss: 0.5618 - accuracy: 0.7676\n14/94 [===&gt;..........................] - ETA: 28s - loss: 0.5620 - accuracy: 0.7664\n15/94 [===&gt;..........................] - ETA: 27s - loss: 0.5646 - accuracy: 0.7625\n16/94 [====&gt;.........................] - ETA: 26s - loss: 0.5658 - accuracy: 0.7604\n17/94 [====&gt;.........................] - ETA: 25s - loss: 0.5680 - accuracy: 0.7574\n18/94 [====&gt;.........................] - ETA: 24s - loss: 0.5713 - accuracy: 0.7535\n19/94 [=====&gt;........................] - ETA: 24s - loss: 0.5768 - accuracy: 0.7478\n20/94 [=====&gt;........................] - ETA: 23s - loss: 0.5782 - accuracy: 0.7458\n21/94 [=====&gt;........................] - ETA: 23s - loss: 0.5752 - accuracy: 0.7480\n22/94 [======&gt;.......................] - ETA: 22s - loss: 0.5745 - accuracy: 0.7481\n23/94 [======&gt;.......................] - ETA: 21s - loss: 0.5710 - accuracy: 0.7509\n24/94 [======&gt;.......................] - ETA: 21s - loss: 0.5734 - accuracy: 0.7483\n25/94 [======&gt;.......................] - ETA: 20s - loss: 0.5730 - accuracy: 0.7483\n26/94 [=======&gt;......................] - ETA: 20s - loss: 0.5791 - accuracy: 0.7420\n27/94 [=======&gt;......................] - ETA: 19s - loss: 0.5833 - accuracy: 0.7377\n28/94 [=======&gt;......................] - ETA: 19s - loss: 0.5817 - accuracy: 0.7388\n29/94 [========&gt;.....................] - ETA: 19s - loss: 0.5863 - accuracy: 0.7342\n30/94 [========&gt;.....................] - ETA: 18s - loss: 0.5826 - accuracy: 0.7375\n31/94 [========&gt;.....................] - ETA: 18s - loss: 0.5797 - accuracy: 0.7399\n32/94 [=========&gt;....................] - ETA: 17s - loss: 0.5784 - accuracy: 0.7409\n33/94 [=========&gt;....................] - ETA: 17s - loss: 0.5765 - accuracy: 0.7424\n34/94 [=========&gt;....................] - ETA: 17s - loss: 0.5786 - accuracy: 0.7402\n35/94 [==========&gt;...................] - ETA: 16s - loss: 0.5769 - accuracy: 0.7417\n36/94 [==========&gt;...................] - ETA: 16s - loss: 0.5759 - accuracy: 0.7425\n37/94 [==========&gt;...................] - ETA: 16s - loss: 0.5762 - accuracy: 0.7421\n38/94 [===========&gt;..................] - ETA: 15s - loss: 0.5735 - accuracy: 0.7445\n39/94 [===========&gt;..................] - ETA: 15s - loss: 0.5730 - accuracy: 0.7447\n40/94 [===========&gt;..................] - ETA: 15s - loss: 0.5714 - accuracy: 0.7458\n41/94 [============&gt;.................] - ETA: 14s - loss: 0.5705 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 14s - loss: 0.5680 - accuracy: 0.7485\n43/94 [============&gt;.................] - ETA: 14s - loss: 0.5703 - accuracy: 0.7461\n44/94 [=============&gt;................] - ETA: 13s - loss: 0.5705 - accuracy: 0.7457\n45/94 [=============&gt;................] - ETA: 13s - loss: 0.5711 - accuracy: 0.7449\n46/94 [=============&gt;................] - ETA: 13s - loss: 0.5709 - accuracy: 0.7450\n47/94 [==============&gt;...............] - ETA: 12s - loss: 0.5692 - accuracy: 0.7465\n48/94 [==============&gt;...............] - ETA: 12s - loss: 0.5684 - accuracy: 0.7470\n49/94 [==============&gt;...............] - ETA: 12s - loss: 0.5696 - accuracy: 0.7457\n50/94 [==============&gt;...............] - ETA: 11s - loss: 0.5694 - accuracy: 0.7458\n51/94 [===============&gt;..............] - ETA: 11s - loss: 0.5687 - accuracy: 0.7463\n52/94 [===============&gt;..............] - ETA: 11s - loss: 0.5680 - accuracy: 0.7468\n53/94 [===============&gt;..............] - ETA: 11s - loss: 0.5672 - accuracy: 0.7472\n54/94 [================&gt;.............] - ETA: 10s - loss: 0.5679 - accuracy: 0.7465\n55/94 [================&gt;.............] - ETA: 10s - loss: 0.5706 - accuracy: 0.7439\n56/94 [================&gt;.............] - ETA: 10s - loss: 0.5719 - accuracy: 0.7426\n57/94 [=================&gt;............] - ETA: 9s - loss: 0.5700 - accuracy: 0.7442 \n58/94 [=================&gt;............] - ETA: 9s - loss: 0.5690 - accuracy: 0.7450\n59/94 [=================&gt;............] - ETA: 9s - loss: 0.5687 - accuracy: 0.7451\n60/94 [==================&gt;...........] - ETA: 9s - loss: 0.5685 - accuracy: 0.7451\n61/94 [==================&gt;...........] - ETA: 8s - loss: 0.5676 - accuracy: 0.7459\n62/94 [==================&gt;...........] - ETA: 8s - loss: 0.5666 - accuracy: 0.7466\n63/94 [===================&gt;..........] - ETA: 8s - loss: 0.5682 - accuracy: 0.7450\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7441\n65/94 [===================&gt;..........] - ETA: 7s - loss: 0.5692 - accuracy: 0.7439\n66/94 [====================&gt;.........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7440\n67/94 [====================&gt;.........] - ETA: 7s - loss: 0.5690 - accuracy: 0.7438\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.5688 - accuracy: 0.7439\n69/94 [=====================&gt;........] - ETA: 6s - loss: 0.5686 - accuracy: 0.7440\n70/94 [=====================&gt;........] - ETA: 6s - loss: 0.5689 - accuracy: 0.7435\n71/94 [=====================&gt;........] - ETA: 6s - loss: 0.5689 - accuracy: 0.7433\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.5700 - accuracy: 0.7419\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.5694 - accuracy: 0.7423\n74/94 [======================&gt;.......] - ETA: 5s - loss: 0.5692 - accuracy: 0.7424\n75/94 [======================&gt;.......] - ETA: 5s - loss: 0.5699 - accuracy: 0.7417\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.5711 - accuracy: 0.7404\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.5716 - accuracy: 0.7397\n78/94 [=======================&gt;......] - ETA: 4s - loss: 0.5722 - accuracy: 0.7390\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.5719 - accuracy: 0.7392\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.5714 - accuracy: 0.7396\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.5700 - accuracy: 0.7410\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.5702 - accuracy: 0.7406\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.5699 - accuracy: 0.7407\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.5696 - accuracy: 0.7408\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.5689 - accuracy: 0.7414\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.5680 - accuracy: 0.7422\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.5684 - accuracy: 0.7416\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.5689 - accuracy: 0.7410\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.5678 - accuracy: 0.7420\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.5664 - accuracy: 0.7433\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7443\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7434\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7440\n94/94 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.7443\n94/94 [==============================] - 27s 272ms/step - loss: 0.5647 - accuracy: 0.7443 - val_loss: 0.5486 - val_accuracy: 0.7444\nEpoch 2/3\n\n 1/94 [..............................] - ETA: 21s - loss: 0.6380 - accuracy: 0.6458\n 2/94 [..............................] - ETA: 21s - loss: 0.5990 - accuracy: 0.6875\n 3/94 [..............................] - ETA: 20s - loss: 0.5941 - accuracy: 0.6944\n 4/94 [&gt;.............................] - ETA: 21s - loss: 0.5791 - accuracy: 0.7083\n 5/94 [&gt;.............................] - ETA: 20s - loss: 0.5733 - accuracy: 0.7125\n 6/94 [&gt;.............................] - ETA: 21s - loss: 0.5583 - accuracy: 0.7257\n 7/94 [=&gt;............................] - ETA: 20s - loss: 0.5513 - accuracy: 0.7321\n 8/94 [=&gt;............................] - ETA: 20s - loss: 0.5540 - accuracy: 0.7292\n 9/94 [=&gt;............................] - ETA: 20s - loss: 0.5646 - accuracy: 0.7199\n10/94 [==&gt;...........................] - ETA: 20s - loss: 0.5613 - accuracy: 0.7229\n11/94 [==&gt;...........................] - ETA: 20s - loss: 0.5524 - accuracy: 0.7311\n12/94 [==&gt;...........................] - ETA: 20s - loss: 0.5463 - accuracy: 0.7361\n13/94 [===&gt;..........................] - ETA: 20s - loss: 0.5535 - accuracy: 0.7292\n14/94 [===&gt;..........................] - ETA: 20s - loss: 0.5599 - accuracy: 0.7217\n15/94 [===&gt;..........................] - ETA: 20s - loss: 0.5504 - accuracy: 0.7306\n16/94 [====&gt;.........................] - ETA: 20s - loss: 0.5449 - accuracy: 0.7357\n17/94 [====&gt;.........................] - ETA: 20s - loss: 0.5377 - accuracy: 0.7426\n18/94 [====&gt;.........................] - ETA: 19s - loss: 0.5388 - accuracy: 0.7407\n19/94 [=====&gt;........................] - ETA: 19s - loss: 0.5453 - accuracy: 0.7346\n20/94 [=====&gt;........................] - ETA: 19s - loss: 0.5474 - accuracy: 0.7323\n21/94 [=====&gt;........................] - ETA: 19s - loss: 0.5447 - accuracy: 0.7341\n22/94 [======&gt;.......................] - ETA: 19s - loss: 0.5416 - accuracy: 0.7367\n23/94 [======&gt;.......................] - ETA: 18s - loss: 0.5372 - accuracy: 0.7400\n24/94 [======&gt;.......................] - ETA: 18s - loss: 0.5376 - accuracy: 0.7396\n25/94 [======&gt;.......................] - ETA: 18s - loss: 0.5334 - accuracy: 0.7433\n26/94 [=======&gt;......................] - ETA: 18s - loss: 0.5279 - accuracy: 0.7484\n27/94 [=======&gt;......................] - ETA: 17s - loss: 0.5275 - accuracy: 0.7485\n28/94 [=======&gt;......................] - ETA: 17s - loss: 0.5281 - accuracy: 0.7470\n29/94 [========&gt;.....................] - ETA: 17s - loss: 0.5288 - accuracy: 0.7457\n30/94 [========&gt;.....................] - ETA: 17s - loss: 0.5288 - accuracy: 0.7451\n31/94 [========&gt;.....................] - ETA: 17s - loss: 0.5277 - accuracy: 0.7460\n32/94 [=========&gt;....................] - ETA: 16s - loss: 0.5271 - accuracy: 0.7461\n33/94 [=========&gt;....................] - ETA: 16s - loss: 0.5287 - accuracy: 0.7443\n34/94 [=========&gt;....................] - ETA: 16s - loss: 0.5287 - accuracy: 0.7439\n35/94 [==========&gt;...................] - ETA: 16s - loss: 0.5277 - accuracy: 0.7440\n36/94 [==========&gt;...................] - ETA: 15s - loss: 0.5267 - accuracy: 0.7448\n37/94 [==========&gt;...................] - ETA: 15s - loss: 0.5225 - accuracy: 0.7483\n38/94 [===========&gt;..................] - ETA: 15s - loss: 0.5226 - accuracy: 0.7473\n39/94 [===========&gt;..................] - ETA: 15s - loss: 0.5221 - accuracy: 0.7468\n40/94 [===========&gt;..................] - ETA: 14s - loss: 0.5217 - accuracy: 0.7469\n41/94 [============&gt;.................] - ETA: 14s - loss: 0.5215 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 14s - loss: 0.5213 - accuracy: 0.7460\n43/94 [============&gt;.................] - ETA: 14s - loss: 0.5211 - accuracy: 0.7452\n44/94 [=============&gt;................] - ETA: 13s - loss: 0.5202 - accuracy: 0.7453\n45/94 [=============&gt;................] - ETA: 13s - loss: 0.5188 - accuracy: 0.7458\n46/94 [=============&gt;................] - ETA: 13s - loss: 0.5202 - accuracy: 0.7437\n47/94 [==============&gt;...............] - ETA: 13s - loss: 0.5184 - accuracy: 0.7438\n48/94 [==============&gt;...............] - ETA: 12s - loss: 0.5173 - accuracy: 0.7444\n49/94 [==============&gt;...............] - ETA: 12s - loss: 0.5162 - accuracy: 0.7445\n50/94 [==============&gt;...............] - ETA: 12s - loss: 0.5167 - accuracy: 0.7437\n51/94 [===============&gt;..............] - ETA: 11s - loss: 0.5171 - accuracy: 0.7422\n52/94 [===============&gt;..............] - ETA: 11s - loss: 0.5150 - accuracy: 0.7436\n53/94 [===============&gt;..............] - ETA: 11s - loss: 0.5149 - accuracy: 0.7429\n54/94 [================&gt;.............] - ETA: 11s - loss: 0.5142 - accuracy: 0.7427\n55/94 [================&gt;.............] - ETA: 10s - loss: 0.5121 - accuracy: 0.7443\n56/94 [================&gt;.............] - ETA: 10s - loss: 0.5124 - accuracy: 0.7422\n57/94 [=================&gt;............] - ETA: 10s - loss: 0.5115 - accuracy: 0.7427\n58/94 [=================&gt;............] - ETA: 10s - loss: 0.5103 - accuracy: 0.7432\n59/94 [=================&gt;............] - ETA: 9s - loss: 0.5089 - accuracy: 0.7440 \n60/94 [==================&gt;...........] - ETA: 9s - loss: 0.5085 - accuracy: 0.7434\n61/94 [==================&gt;...........] - ETA: 9s - loss: 0.5076 - accuracy: 0.7439\n62/94 [==================&gt;...........] - ETA: 8s - loss: 0.5066 - accuracy: 0.7440\n63/94 [===================&gt;..........] - ETA: 8s - loss: 0.5067 - accuracy: 0.7421\n64/94 [===================&gt;..........] - ETA: 8s - loss: 0.5058 - accuracy: 0.7425\n65/94 [===================&gt;..........] - ETA: 8s - loss: 0.5044 - accuracy: 0.7436\n66/94 [====================&gt;.........] - ETA: 7s - loss: 0.5031 - accuracy: 0.7434\n67/94 [====================&gt;.........] - ETA: 7s - loss: 0.5012 - accuracy: 0.7447\n68/94 [====================&gt;.........] - ETA: 7s - loss: 0.5005 - accuracy: 0.7436\n69/94 [=====================&gt;........] - ETA: 7s - loss: 0.5011 - accuracy: 0.7412\n70/94 [=====================&gt;........] - ETA: 6s - loss: 0.4998 - accuracy: 0.7417\n71/94 [=====================&gt;........] - ETA: 6s - loss: 0.4972 - accuracy: 0.7435\n72/94 [=====================&gt;........] - ETA: 6s - loss: 0.4966 - accuracy: 0.7431\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.4949 - accuracy: 0.7443\n74/94 [======================&gt;.......] - ETA: 5s - loss: 0.4950 - accuracy: 0.7430\n75/94 [======================&gt;.......] - ETA: 5s - loss: 0.4929 - accuracy: 0.7444\n76/94 [=======================&gt;......] - ETA: 5s - loss: 0.4910 - accuracy: 0.7462\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.4900 - accuracy: 0.7465\n78/94 [=======================&gt;......] - ETA: 4s - loss: 0.4891 - accuracy: 0.7468\n79/94 [========================&gt;.....] - ETA: 4s - loss: 0.4888 - accuracy: 0.7458\n80/94 [========================&gt;.....] - ETA: 4s - loss: 0.4883 - accuracy: 0.7453\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.4880 - accuracy: 0.7454\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.4859 - accuracy: 0.7472\n83/94 [=========================&gt;....] - ETA: 3s - loss: 0.4845 - accuracy: 0.7482\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.4840 - accuracy: 0.7478\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.4835 - accuracy: 0.7473\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.4815 - accuracy: 0.7478\n87/94 [==========================&gt;...] - ETA: 2s - loss: 0.4813 - accuracy: 0.7476\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.4803 - accuracy: 0.7476\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.4788 - accuracy: 0.7488\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.4780 - accuracy: 0.7488\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.4774 - accuracy: 0.7491\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7486\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7498\n94/94 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.7501\n94/94 [==============================] - 29s 308ms/step - loss: 0.4745 - accuracy: 0.7501 - val_loss: 0.3965 - val_accuracy: 0.7998\nEpoch 3/3\n\n 1/94 [..............................] - ETA: 27s - loss: 0.3033 - accuracy: 0.8958\n 2/94 [..............................] - ETA: 28s - loss: 0.3199 - accuracy: 0.8750\n 3/94 [..............................] - ETA: 28s - loss: 0.3137 - accuracy: 0.8819\n 4/94 [&gt;.............................] - ETA: 30s - loss: 0.3038 - accuracy: 0.8698\n 5/94 [&gt;.............................] - ETA: 32s - loss: 0.3082 - accuracy: 0.8583\n 6/94 [&gt;.............................] - ETA: 31s - loss: 0.3238 - accuracy: 0.8438\n 7/94 [=&gt;............................] - ETA: 30s - loss: 0.3133 - accuracy: 0.8542\n 8/94 [=&gt;............................] - ETA: 29s - loss: 0.3192 - accuracy: 0.8438\n 9/94 [=&gt;............................] - ETA: 29s - loss: 0.3184 - accuracy: 0.8495\n10/94 [==&gt;...........................] - ETA: 30s - loss: 0.3140 - accuracy: 0.8542\n11/94 [==&gt;...........................] - ETA: 29s - loss: 0.3094 - accuracy: 0.8485\n12/94 [==&gt;...........................] - ETA: 29s - loss: 0.3068 - accuracy: 0.8490\n13/94 [===&gt;..........................] - ETA: 28s - loss: 0.3045 - accuracy: 0.8526\n14/94 [===&gt;..........................] - ETA: 28s - loss: 0.3027 - accuracy: 0.8542\n15/94 [===&gt;..........................] - ETA: 28s - loss: 0.2993 - accuracy: 0.8556\n16/94 [====&gt;.........................] - ETA: 27s - loss: 0.2950 - accuracy: 0.8607\n17/94 [====&gt;.........................] - ETA: 27s - loss: 0.2927 - accuracy: 0.8640\n18/94 [====&gt;.........................] - ETA: 27s - loss: 0.2916 - accuracy: 0.8657\n19/94 [=====&gt;........................] - ETA: 26s - loss: 0.2937 - accuracy: 0.8618\n20/94 [=====&gt;........................] - ETA: 26s - loss: 0.2921 - accuracy: 0.8635\n21/94 [=====&gt;........................] - ETA: 25s - loss: 0.2894 - accuracy: 0.8631\n22/94 [======&gt;.......................] - ETA: 25s - loss: 0.2855 - accuracy: 0.8655\n23/94 [======&gt;.......................] - ETA: 24s - loss: 0.2826 - accuracy: 0.8659\n24/94 [======&gt;.......................] - ETA: 24s - loss: 0.2806 - accuracy: 0.8672\n25/94 [======&gt;.......................] - ETA: 23s - loss: 0.2821 - accuracy: 0.8650\n26/94 [=======&gt;......................] - ETA: 23s - loss: 0.2804 - accuracy: 0.8646\n27/94 [=======&gt;......................] - ETA: 22s - loss: 0.2792 - accuracy: 0.8657\n28/94 [=======&gt;......................] - ETA: 22s - loss: 0.2804 - accuracy: 0.8631\n29/94 [========&gt;.....................] - ETA: 21s - loss: 0.2777 - accuracy: 0.8657\n30/94 [========&gt;.....................] - ETA: 21s - loss: 0.2797 - accuracy: 0.8632\n31/94 [========&gt;.....................] - ETA: 21s - loss: 0.2790 - accuracy: 0.8649\n32/94 [=========&gt;....................] - ETA: 20s - loss: 0.2754 - accuracy: 0.8672\n33/94 [=========&gt;....................] - ETA: 20s - loss: 0.2733 - accuracy: 0.8699\n34/94 [=========&gt;....................] - ETA: 19s - loss: 0.2713 - accuracy: 0.8713\n35/94 [==========&gt;...................] - ETA: 19s - loss: 0.2705 - accuracy: 0.8726\n36/94 [==========&gt;...................] - ETA: 19s - loss: 0.2685 - accuracy: 0.8738\n37/94 [==========&gt;...................] - ETA: 18s - loss: 0.2671 - accuracy: 0.8761\n38/94 [===========&gt;..................] - ETA: 18s - loss: 0.2663 - accuracy: 0.8750\n39/94 [===========&gt;..................] - ETA: 18s - loss: 0.2642 - accuracy: 0.8766\n40/94 [===========&gt;..................] - ETA: 17s - loss: 0.2629 - accuracy: 0.8781\n41/94 [============&gt;.................] - ETA: 17s - loss: 0.2622 - accuracy: 0.8775\n42/94 [============&gt;.................] - ETA: 17s - loss: 0.2600 - accuracy: 0.8800\n43/94 [============&gt;.................] - ETA: 16s - loss: 0.2621 - accuracy: 0.8789\n44/94 [=============&gt;................] - ETA: 16s - loss: 0.2628 - accuracy: 0.8788\n45/94 [=============&gt;................] - ETA: 16s - loss: 0.2613 - accuracy: 0.8796\n46/94 [=============&gt;................] - ETA: 15s - loss: 0.2600 - accuracy: 0.8786\n47/94 [==============&gt;...............] - ETA: 15s - loss: 0.2582 - accuracy: 0.8790\n48/94 [==============&gt;...............] - ETA: 15s - loss: 0.2571 - accuracy: 0.8793\n49/94 [==============&gt;...............] - ETA: 14s - loss: 0.2566 - accuracy: 0.8797\n50/94 [==============&gt;...............] - ETA: 14s - loss: 0.2568 - accuracy: 0.8788\n51/94 [===============&gt;..............] - ETA: 14s - loss: 0.2557 - accuracy: 0.8795\n52/94 [===============&gt;..............] - ETA: 13s - loss: 0.2562 - accuracy: 0.8786\n53/94 [===============&gt;..............] - ETA: 13s - loss: 0.2548 - accuracy: 0.8801\n54/94 [================&gt;.............] - ETA: 13s - loss: 0.2524 - accuracy: 0.8819\n55/94 [================&gt;.............] - ETA: 12s - loss: 0.2509 - accuracy: 0.8833\n56/94 [================&gt;.............] - ETA: 12s - loss: 0.2505 - accuracy: 0.8847\n57/94 [=================&gt;............] - ETA: 11s - loss: 0.2518 - accuracy: 0.8834\n58/94 [=================&gt;............] - ETA: 11s - loss: 0.2502 - accuracy: 0.8840\n59/94 [=================&gt;............] - ETA: 11s - loss: 0.2495 - accuracy: 0.8845\n60/94 [==================&gt;...........] - ETA: 10s - loss: 0.2482 - accuracy: 0.8851\n61/94 [==================&gt;...........] - ETA: 10s - loss: 0.2473 - accuracy: 0.8852\n62/94 [==================&gt;...........] - ETA: 10s - loss: 0.2463 - accuracy: 0.8861\n63/94 [===================&gt;..........] - ETA: 9s - loss: 0.2461 - accuracy: 0.8856 \n64/94 [===================&gt;..........] - ETA: 9s - loss: 0.2446 - accuracy: 0.8870\n65/94 [===================&gt;..........] - ETA: 9s - loss: 0.2431 - accuracy: 0.8878\n66/94 [====================&gt;.........] - ETA: 8s - loss: 0.2420 - accuracy: 0.8889\n67/94 [====================&gt;.........] - ETA: 8s - loss: 0.2420 - accuracy: 0.8887\n68/94 [====================&gt;.........] - ETA: 8s - loss: 0.2401 - accuracy: 0.8900\n69/94 [=====================&gt;........] - ETA: 7s - loss: 0.2406 - accuracy: 0.8895\n70/94 [=====================&gt;........] - ETA: 7s - loss: 0.2386 - accuracy: 0.8905\n71/94 [=====================&gt;........] - ETA: 7s - loss: 0.2376 - accuracy: 0.8911\n72/94 [=====================&gt;........] - ETA: 6s - loss: 0.2365 - accuracy: 0.8918\n73/94 [======================&gt;.......] - ETA: 6s - loss: 0.2360 - accuracy: 0.8918\n74/94 [======================&gt;.......] - ETA: 6s - loss: 0.2354 - accuracy: 0.8922\n75/94 [======================&gt;.......] - ETA: 6s - loss: 0.2341 - accuracy: 0.8928\n76/94 [=======================&gt;......] - ETA: 5s - loss: 0.2318 - accuracy: 0.8942\n77/94 [=======================&gt;......] - ETA: 5s - loss: 0.2297 - accuracy: 0.8956\n78/94 [=======================&gt;......] - ETA: 5s - loss: 0.2284 - accuracy: 0.8958\n79/94 [========================&gt;.....] - ETA: 4s - loss: 0.2277 - accuracy: 0.8961\n80/94 [========================&gt;.....] - ETA: 4s - loss: 0.2269 - accuracy: 0.8966\n81/94 [========================&gt;.....] - ETA: 4s - loss: 0.2265 - accuracy: 0.8963\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.2257 - accuracy: 0.8966\n83/94 [=========================&gt;....] - ETA: 3s - loss: 0.2259 - accuracy: 0.8966\n84/94 [=========================&gt;....] - ETA: 3s - loss: 0.2249 - accuracy: 0.8971\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.2235 - accuracy: 0.8978\n86/94 [==========================&gt;...] - ETA: 2s - loss: 0.2222 - accuracy: 0.8985\n87/94 [==========================&gt;...] - ETA: 2s - loss: 0.2208 - accuracy: 0.8994\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.2199 - accuracy: 0.9001\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.2198 - accuracy: 0.9000\n90/94 [===========================&gt;..] - ETA: 1s - loss: 0.2200 - accuracy: 0.9002\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9011\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9010\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9017\n94/94 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9017\n94/94 [==============================] - 30s 318ms/step - loss: 0.2173 - accuracy: 0.9017 - val_loss: 0.3078 - val_accuracy: 0.8811\n&lt;keras.src.callbacks.History object at 0x0000024435E3FE10&gt;\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\n\ny_pred_probs = model.predict(X_test)\n\n\n 1/35 [..............................] - ETA: 7s\n 4/35 [==&gt;...........................] - ETA: 0s\n 7/35 [=====&gt;........................] - ETA: 0s\n10/35 [=======&gt;......................] - ETA: 0s\n13/35 [==========&gt;...................] - ETA: 0s\n15/35 [===========&gt;..................] - ETA: 0s\n18/35 [==============&gt;...............] - ETA: 0s\n21/35 [=================&gt;............] - ETA: 0s\n24/35 [===================&gt;..........] - ETA: 0s\n27/35 [======================&gt;.......] - ETA: 0s\n30/35 [========================&gt;.....] - ETA: 0s\n33/35 [===========================&gt;..] - ETA: 0s\n35/35 [==============================] - 1s 21ms/step\n\ny_pred = (model.predict(X_test) &gt; 0.5).astype(\"int32\")\n\n\n 1/35 [..............................] - ETA: 2s\n 4/35 [==&gt;...........................] - ETA: 0s\n 7/35 [=====&gt;........................] - ETA: 0s\n11/35 [========&gt;.....................] - ETA: 0s\n14/35 [===========&gt;..................] - ETA: 0s\n17/35 [=============&gt;................] - ETA: 0s\n20/35 [================&gt;.............] - ETA: 0s\n23/35 [==================&gt;...........] - ETA: 0s\n26/35 [=====================&gt;........] - ETA: 0s\n28/35 [=======================&gt;......] - ETA: 0s\n32/35 [==========================&gt;...] - ETA: 0s\n35/35 [==============================] - ETA: 0s\n35/35 [==============================] - 1s 21ms/step\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy}\")\n\nTest Accuracy: 0.8811438784629133\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\n\nConfusion Matrix:\n\nprint(conf_matrix)\n\n[[812  21]\n [112 174]]\n\n\nDas neuronale Netzwerk sagt das Train-Sample zwar perfekt vorher, hat jedoch vergleichsweise gro√üe Schwierigkeiten beim Test-Sample."
  },
  {
    "objectID": "posts/Hatespeech_Twitter.html#fazit",
    "href": "posts/Hatespeech_Twitter.html#fazit",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Durch die explorative Datenanalyse war es m√∂glich, einige relevante Charakteristika herauszuarbeiten, die Hate-Speech-Tweets klar von anderen Tweets abgrenzen. Hate Speech enth√§lt n√§mlich einen gro√üen Anteil an Beleidigungen und Schimpfw√∂rtern sowie negativen Sentimenten. Diese Erkenntnisse waren f√ºr die Modellierung hilfreich, da es gelang, Features basierend auf der EDA zu generieren, die von hoher Relevanz f√ºr die Performance des Modells waren. Das Ziel der Modellierung war es, ein Modell zu trainieren, das Hate Speech in Tweets m√∂glichst akkurat erkennt. Durch den kombinierten Ansatz aus Training und Deep Learning wurde dieses Ziel mit Erfolg erreicht, auch wenn die Deep Learning Modelle vergleichsweise schlecht abschnitten."
  }
]